%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


% Jupyter Notebook code cell colors
\definecolor{nbsphinxin}{HTML}{307FC1}
\definecolor{nbsphinxout}{HTML}{BF5B3D}
\definecolor{nbsphinx-code-bg}{HTML}{F5F5F5}
\definecolor{nbsphinx-code-border}{HTML}{E0E0E0}
\definecolor{nbsphinx-stderr}{HTML}{FFDDDD}
% ANSI colors for output streams and traceback highlighting
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% Define an environment for non-plain-text code cell outputs (e.g. images)
\makeatletter
\newenvironment{nbsphinxfancyoutput}{%
    % Avoid fatal error with framed.sty if graphics too long to fit on one page
    \let\sphinxincludegraphics\nbsphinxincludegraphics
    \nbsphinx@image@maxheight\textheight
    \advance\nbsphinx@image@maxheight -2\fboxsep   % default \fboxsep 3pt
    \advance\nbsphinx@image@maxheight -2\fboxrule  % default \fboxrule 0.4pt
    \advance\nbsphinx@image@maxheight -\baselineskip
\def\nbsphinxfcolorbox{\spx@fcolorbox{nbsphinx-code-border}{white}}%
\def\FrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\@empty}%
\def\FirstFrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\sphinxVerbatim@Continues}%
\def\MidFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\sphinxVerbatim@Continues}%
\def\LastFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\@empty}%
\MakeFramed{\advance\hsize-\width\@totalleftmargin\z@\linewidth\hsize\@setminipage}%
\lineskip=1ex\lineskiplimit=1ex\raggedright%
}{\par\unskip\@minipagefalse\endMakeFramed}
\makeatother
\newbox\nbsphinxpromptbox
\def\nbsphinxfancyaddprompt{\ifvoid\nbsphinxpromptbox\else
    \kern\fboxrule\kern\fboxsep
    \copy\nbsphinxpromptbox
    \kern-\ht\nbsphinxpromptbox\kern-\dp\nbsphinxpromptbox
    \kern-\fboxsep\kern-\fboxrule\nointerlineskip
    \fi}
\newlength\nbsphinxcodecellspacing
\setlength{\nbsphinxcodecellspacing}{0pt}

% Define support macros for attaching opening and closing lines to notebooks
\newsavebox\nbsphinxbox
\makeatletter
\newcommand{\nbsphinxstartnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vtop{{#1\par}}
    % reserve some space at bottom of page, else start new page
    \needspace{\dimexpr2.5\baselineskip+\ht\nbsphinxbox+\dp\nbsphinxbox}
    % mimick vertical spacing from \section command
      \addpenalty\@secpenalty
      \@tempskipa 3.5ex \@plus 1ex \@minus .2ex\relax
      \addvspace\@tempskipa
      {\Large\@tempskipa\baselineskip
             \advance\@tempskipa-\prevdepth
             \advance\@tempskipa-\ht\nbsphinxbox
             \ifdim\@tempskipa>\z@
               \vskip \@tempskipa
             \fi}
    \unvbox\nbsphinxbox
    % if notebook starts with a \section, prevent it from adding extra space
    \@nobreaktrue\everypar{\@nobreakfalse\everypar{}}%
    % compensate the parskip which will get inserted by next paragraph
    \nobreak\vskip-\parskip
    % do not break here
    \nobreak
}% end of \nbsphinxstartnotebook

\newcommand{\nbsphinxstopnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vbox{{#1\par}}
    \nobreak % it updates page totals
    \dimen@\pagegoal
    \advance\dimen@-\pagetotal \advance\dimen@-\pagedepth
    \advance\dimen@-\ht\nbsphinxbox \advance\dimen@-\dp\nbsphinxbox
    \ifdim\dimen@<\z@
      % little space left
      \unvbox\nbsphinxbox
      \kern-.8\baselineskip
      \nobreak\vskip\z@\@plus1fil
      \penalty100
      \vskip\z@\@plus-1fil
      \kern.8\baselineskip
    \else
      \unvbox\nbsphinxbox
    \fi
}% end of \nbsphinxstopnotebook

% Ensure height of an included graphics fits in nbsphinxfancyoutput frame
\newdimen\nbsphinx@image@maxheight % set in nbsphinxfancyoutput environment
\newcommand*{\nbsphinxincludegraphics}[2][]{%
    \gdef\spx@includegraphics@options{#1}%
    \setbox\spx@image@box\hbox{\includegraphics[#1,draft]{#2}}%
    \in@false
    \ifdim \wd\spx@image@box>\linewidth
      \g@addto@macro\spx@includegraphics@options{,width=\linewidth}%
      \in@true
    \fi
    % no rotation, no need to worry about depth
    \ifdim \ht\spx@image@box>\nbsphinx@image@maxheight
      \g@addto@macro\spx@includegraphics@options{,height=\nbsphinx@image@maxheight}%
      \in@true
    \fi
    \ifin@
      \g@addto@macro\spx@includegraphics@options{,keepaspectratio}%
    \fi
    \setbox\spx@image@box\box\voidb@x % clear memory
    \expandafter\includegraphics\expandafter[\spx@includegraphics@options]{#2}%
}% end of "\MakeFrame"-safe variant of \sphinxincludegraphics
\makeatother

\makeatletter
\renewcommand*\sphinx@verbatim@nolig@list{\do\'\do\`}
\begingroup
\catcode`'=\active
\let\nbsphinx@noligs\@noligs
\g@addto@macro\nbsphinx@noligs{\let'\PYGZsq}
\endgroup
\makeatother
\renewcommand*\sphinxbreaksbeforeactivelist{\do\<\do\"\do\'}
\renewcommand*\sphinxbreaksafteractivelist{\do\.\do\,\do\:\do\;\do\?\do\!\do\/\do\>\do\-}
\makeatletter
\fvset{codes*=\sphinxbreaksattexescapedchars\do\^\^\let\@noligs\nbsphinx@noligs}
\makeatother



\title{Implementation of a framework to fine\sphinxhyphen{}tune GPT/GPT2 based models on abstractive summarization.\@{}}
\date{Jul 05, 2022}
\release{}
\author{David Lorenzo Alfaro}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
Abstractive summarization has experienced a surge of interest thanks to recent advancements on Transformer\sphinxhyphen{}based encoder\sphinxhyphen{}decoder models, with standout proposals like PEGASUS, that incorporates explicit pre\sphinxhyphen{}training objectives tailored for such a task, exhibiting unprecedented level of performance on natural language generation tasks. However, the humongous amount of data required and the massive computational cost attached to the pre\sphinxhyphen{}training of these architectures imposes a substantial burden on their availability in languages other than English, with the very exception of their multi\sphinxhyphen{}lingual homologous.

\sphinxAtStartPar
The recent large Spanish language models from the MarIA project, based on the RoBERTa and GPT\sphinxhyphen{}2 architectures, have shown promising results, pushing the state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art on multiple natural language understanding tasks. However, encoder\sphinxhyphen{} and decoder\sphinxhyphen{}only systems pose as an architecturally suboptimal approach to resolve sequence\sphinxhyphen{}to\sphinxhyphen{}sequence tasks. In this work, we explore the applicability of these language models for abstractive summarization.

\sphinxAtStartPar
In this page, we present the documentation of a fully\sphinxhyphen{}documented API\sphinxhyphen{}like tool to fine\sphinxhyphen{}tune GPT\sphinxhyphen{}like architectures on abstractive summarization.


\chapter{Exploratory Data Analysis of the \sphinxstyleemphasis{XL\sphinxhyphen{}Sum} dataset.}
\label{\detokenize{eda:Exploratory-Data-Analysis-of-the-XL-Sum-dataset.}}\label{\detokenize{eda::doc}}
\sphinxAtStartPar
\sphinxstylestrong{As part of the MSc. dissertation: Applying large Spanish language models to Natural Language Processing tasks.}

\sphinxAtStartPar
David Lorenzo Alfaro

\sphinxAtStartPar
July, 2022

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[28]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}\PYG{p}{,} \PYG{n}{GPT2LMHeadModel}\PYG{p}{,} \PYG{n}{RobertaForMaskedLM}
\PYG{k+kn}{from} \PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{get\PYGZus{}tokenized\PYGZus{}text}
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{Iterable}\PYG{p}{,} \PYG{n}{Union}
\PYG{k+kn}{from} \PYG{n+nn}{pathlib} \PYG{k+kn}{import} \PYG{n}{Path}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Preliminary data analysis}
\label{\detokenize{eda:Preliminary-data-analysis}}
\sphinxAtStartPar
Let us first load the data from disk. It is a prerrequisite to either have it downloaded locally and located in a subdirectory called \sphinxcode{\sphinxupquote{summaries/}}, or make the appropriate modifications to the code to load it (e.g., via the HuggingFace’s datasets library, \sphinxcode{\sphinxupquote{wget}} request/s). In particular, we gathered the data from the \sphinxstyleemphasis{XL\sphinxhyphen{}Sum} official \sphinxhref{https://github.com/csebuetnlp/xl-sum}{GitHub repository}

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[10]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{DATA\PYGZus{}DIR} \PYG{o}{=} \PYG{n}{Path}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{summaries/all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{TRAIN\PYGZus{}PATH} \PYG{o}{=} \PYG{n}{Path}\PYG{p}{(}\PYG{n}{DATA\PYGZus{}DIR}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train.jsonl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{VAL\PYGZus{}PATH} \PYG{o}{=} \PYG{n}{Path}\PYG{p}{(}\PYG{n}{DATA\PYGZus{}DIR}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val.jsonl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{TEST\PYGZus{}PATH} \PYG{o}{=} \PYG{n}{Path}\PYG{p}{(}\PYG{n}{DATA\PYGZus{}DIR}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test.jsonl}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The necessary columns in the dataset are as listed: * \sphinxcode{\sphinxupquote{text}}: content of an article. * \sphinxcode{\sphinxupquote{summary}}: summary of an article. * \sphinxcode{\sphinxupquote{id}}: unique identifier of the article.

\sphinxAtStartPar
Indeed, other than article\sphinxhyphen{}summary pairs, all remaining information may be disregarded for the task that we are aimed at conducting.

\sphinxAtStartPar
Let us first load the training dataset to inspect some of its properties

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[11]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{df\PYGZus{}train} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}json}\PYG{p}{(}\PYG{n}{TRAIN\PYGZus{}PATH}\PYG{p}{,} \PYG{n}{lines}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{summary}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Now, let us print the first \(k\) elements of the training dataset

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[12]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{top\PYGZus{}k}\PYG{o}{=}\PYG{l+m+mi}{3}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{p}{(}\PYG{n}{idx}\PYG{p}{,} \PYG{n}{s}\PYG{p}{,}\PYG{n}{t}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{id}\PYG{p}{,} \PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{text}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{top\PYGZus{}k}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{idx}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Text: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Summary: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{s}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
id:140930\_ultnot\_siria\_onu\_comida\_ch
Text: De no recibir más dinero, las raciones de la ONU en Siria se terminarán en dos meses. En un informe al Consejo de Seguridad de la ONU, Amos dijo que las raciones del PMA destinadas a los 4.000.000 de sirios ya han sido recortadas para poder llegar a la mayor cantidad de personas posible. Amos también hizo un llamado a juntar suministros para proteger a los sirios del frío, en miras al próximo invierno.
Summary: La directora de ayuda humanitaria de Naciones Unidas, Valerie Amos, advirtió que de no invertir más dinero, el Programa Mundial de Alimentos tendrá que detener sus operaciones en Siria en dos meses.

id:130809\_ultnot\_protesta\_cachemira\_protesa\_aa
Text: Manifestantes hindúes gritan consignas contra el gobierno en la región de Cachemira administrada por India. Las manifestaciones tuvieron lugar después de los rezos especiales Eid, en la ciudad de Srinagar y en diversas ciudades de la región. La policía dice que la respuesta se produjo cuando la muchedumbre se volvió violenta y empezó a lanzarles palos y piedras. Varios policías y manifestantes resultaron heridos. El jueves a la noche diversos líderes separatistas fueron arrestados para evitar que lideraran las protestas.
Summary: La policía en la región de Cachemira administrada por India, lanzó gas lacrimógeno y disparó balas de goma para dispersar una protesa contra supuestas violaciones de los derechos humanos llevadas a cabo por las fuerzas del gobierno.

id:media-37220890
Text: Así lo vemos en esta animación que muestra cómo el río busca nuevos caminos. Si bien el mundo ha perdido millones de litros de agua, también se han ganado 115.000 kilómetros cuadrados. La zona donde ha habido un mayor aumento de agua en el mundo es en la meseta tibetana (donde el derretimiento de glaciares está creando lagos). Pero es en el Amazonas donde esta lucha es más evidente.
Summary: La naturaleza es un hueso duro de roer.

\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Alternatively, one can resort to the built\sphinxhyphen{}in function \sphinxcode{\sphinxupquote{head}} available in \sphinxcode{\sphinxupquote{pd.DataFrame}} objects.

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[13]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{n}{top\PYGZus{}k}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[13]:\,\hspace{\fboxrule}\hspace{\fboxsep}}                                            id  \textbackslash{}
0            140930\_ultnot\_siria\_onu\_comida\_ch
1  130809\_ultnot\_protesta\_cachemira\_protesa\_aa
2                               media-37220890

                                             summary  \textbackslash{}
0  La directora de ayuda humanitaria de Naciones {\ldots}
1  La policía en la región de Cachemira administr{\ldots}
2            La naturaleza es un hueso duro de roer.

                                                text
0  De no recibir más dinero, las raciones de la O{\ldots}
1  Manifestantes hindúes gritan consignas contra {\ldots}
2  Así lo vemos en esta animación que muestra cóm{\ldots}
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Impact of the limited dimensionality of the Language Models for Abstractive Summarization}
\label{\detokenize{eda:Impact-of-the-limited-dimensionality-of-the-Language-Models-for-Abstractive-Summarization}}
\sphinxAtStartPar
Both GPT\sphinxhyphen{}2 and RoBERTa work on the subword level. All spanish language models from the MarIA project have a limited dimensionality of 512 tokens (or subwords). Furthermore, in order to generate summaries on a decoder\sphinxhyphen{}only system, both the text and the summary must fit into the model, which imposes further restrictions on the amount of instances in the dataset that can be processed in as as\sphinxhyphen{}is fashion (i.e., without needing to do any further modification in the text/summary of those instances
before feeding them into the model). In summarization tasks, where the prominent practice is to strive for models with larger dimensionalities, this shortcoming is of vital relevance because it necessarily constraints the capabilities of the models to work with large texts.


\subsection{Training set}
\label{\detokenize{eda:Training-set}}
\sphinxAtStartPar
Let us inspect the number of \sphinxstyleemphasis{valid} instances (those that fit into the model) in the training set of data, using the GPT\sphinxhyphen{}2 model, and a RoBERTa2RoBERTa (a.k.a. RoBERTaShared) model. To that end, we will compute the number of article and summary tokens using the GPT\sphinxhyphen{}2 tokenizer for the spanish LM, and the number of article tokens using the RoBERTa tokenizer. Note that there is no need to compute the per\sphinxhyphen{}summary no. of tokens because this model will be subsequently used as the \sphinxstyleemphasis{warmed\sphinxhyphen{}up} models
of an encoder\sphinxhyphen{}decoder system. However, we will also yield this statistic in order to further study some properties about the length of the summaries, which can be of vital relevance in the generation of the summaries.

\sphinxAtStartPar
Beforehand, let us define some useful functions that will enable us to comptue the number of tokens of a collection of tokens using a tokenizer (\sphinxcode{\sphinxupquote{calc\_document\_tokens}}), and to plot histograms for article and summary length (also in terms of subwords) and a scatterplot relating these two random variables.

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[57]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k}{def} \PYG{n+nf}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{documents}\PYG{p}{:}\PYG{n}{Iterable}\PYG{p}{,} \PYG{n}{tokenizer}\PYG{p}{:}\PYG{n}{Union}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{Path}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Compute the number of tokens of a collection of documents using a pretrained tokenizer}

\PYG{l+s+sd}{    Parameters}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    documents : Iterable}
\PYG{l+s+sd}{        Collection of documents}

\PYG{l+s+sd}{    tokenizer : Union[str, Path]}
\PYG{l+s+sd}{        Model identifier of a predefined tokenizer hosted inside a model repo}
\PYG{l+s+sd}{         on huggingface.co}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{c+c1}{\PYGZsh{} Load tokenizer}
    \PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{p}{)}

    \PYG{k}{return} \PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{get\PYGZus{}tokenized\PYGZus{}text}\PYG{p}{(}\PYG{n}{doc}\PYG{p}{,} \PYG{n}{tokenizer}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{doc} \PYG{o+ow}{in} \PYG{n}{documents}\PYG{p}{]}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}stats}\PYG{p}{(}\PYG{n}{article\PYGZus{}tokens}\PYG{p}{:}\PYG{n}{Iterable}\PYG{p}{,} \PYG{n}{summary\PYGZus{}tokens}\PYG{p}{:}\PYG{n}{Iterable}\PYG{p}{,} \PYG{n}{hist\PYGZus{}bins}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{scatter\PYGZus{}marker\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Plot some statistics about the article and summary tokens.}

\PYG{l+s+sd}{    Parameters}
\PYG{l+s+sd}{    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{l+s+sd}{    article\PYGZus{}tokens : Iterable}
\PYG{l+s+sd}{        Collection of per\PYGZhy{}sample article tokens}

\PYG{l+s+sd}{    summary\PYGZus{}tokens : Iterable}
\PYG{l+s+sd}{        Collection of per\PYGZhy{}sample summary tokens}

\PYG{l+s+sd}{    hist\PYGZus{}bins : int}
\PYG{l+s+sd}{        Number of bins used to discretize data for histograms, defaults to 20}

\PYG{l+s+sd}{    scatter\PYGZus{}marker\PYGZus{}size : int}
\PYG{l+s+sd}{        Marker size of the scatter\PYGZhy{}plot, defaults to 3}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{fig}\PYG{p}{,} \PYG{p}{(}\PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}\PYG{p}{,} \PYG{n}{ax3}\PYG{p}{)} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{ncols}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{article\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{n}{hist\PYGZus{}bins}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{summary\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{n}{hist\PYGZus{}bins}\PYG{p}{)}
    \PYG{n}{ax3}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{article\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{summary\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{n}{scatter\PYGZus{}marker\PYGZus{}size}\PYG{p}{)}
    \PYG{n}{fig}\PYG{o}{.}\PYG{n}{set\PYGZus{}size\PYGZus{}inches}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
    \PYG{n}{fig}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Now, let us compute the of tokens for the articles and summaries using the tokenizers of the GPT\sphinxhyphen{}2 and RoBERTa checkpoints.

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[17]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{checkpoint\PYGZus{}gpt2} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PlanTL\PYGZhy{}GOB\PYGZhy{}ES/gpt2\PYGZhy{}base\PYGZhy{}bne}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}
\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{stderr}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[54]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{checkpoint\PYGZus{}roberta} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PlanTL\PYGZhy{}GOB\PYGZhy{}ES/roberta\PYGZhy{}base\PYGZhy{}bne}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\PYG{n}{roberta\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{stderr}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Token indices sequence length is longer than the specified maximum sequence length for this model (2183 > 512). Running this sequence through the model will result in indexing errors
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
To perform fine\sphinxhyphen{}tuning on a GPT\sphinxhyphen{}2 based model, at least 1 token should be reserved for a special token, which will serve as a separator between the article and the summary of each input sample. As you may see from the training sources, our particular take is to use the \sphinxcode{\sphinxupquote{\textless{}|sep|\textgreater{}}} gram as the separator token, albeit a different sequence may be used, so long it does not conflict with an existing entry in the vocabulary of the tokenizer.

\sphinxAtStartPar
On the other hand, although we that the input maximum length is constraint to 512 tokens, a programmatic way to check such limitation is by accessing to the \sphinxcode{\sphinxupquote{config.n\_positions}} property of a GPT\sphinxhyphen{}2 HuggingFace checkpoint.

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[40]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens} \PYG{o}{=} \PYG{n}{GPT2LMHeadModel}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{config}\PYG{o}{.}\PYG{n}{n\PYGZus{}positions}
\PYG{n}{gpt2\PYGZus{}valid\PYGZus{}train} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{gpt2\PYGZus{}invalid\PYGZus{}train} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{gpt2\PYGZus{}valid\PYGZus{}train}
\PYG{c+c1}{\PYGZsh{} out of the box... how many articles can be processed by the gpt\PYGZhy{}2 model?}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Using the }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{ model, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}valid\PYGZus{}train}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ out of }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ training samples have \PYGZlt{}= }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ tokens and thus are subject to application out of the box, whereas }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}invalid\PYGZus{}train}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ will cause an error unless further measures are taken (e.g., via truncation of info. in a meaningful fashion).}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Using the "PlanTL-GOB-ES/gpt2-base-bne" model, 9692 out of 38110 training samples have <= 511 tokens and thus are subject to application out of the box, whereas 28418 will cause an error unless further measures are taken (e.g., via truncation of info. in a meaningful fashion).
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Let us further inspect some properties about the samples in the training dataset: * How are the length of the articles and summaries distributed? * Are the length of the article and its summary correlated?

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[58]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}stats}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{,} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}
\noindent\sphinxincludegraphics[width=848\sphinxpxdimen,height=280\sphinxpxdimen]{{eda_22_0}.png}

\end{sphinxuseclass}
\end{sphinxuseclass}
\end{nbsphinxfancyoutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
From the graphs we can tell: * vast majority of the articles are around 500 to 1500 tokens long, whereas regular summary length revolves around 20 to 50 tokens. * summary length seems to be independent from the length of the article.

\sphinxAtStartPar
We can also calculate some descriptive statistics summarizing the central tendency, dispersion and shape of the distribution of the article and summary tokens.

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[68]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[68]:\,\hspace{\fboxrule}\hspace{\fboxsep}}       gpt2\_article\_tokens\_train  gpt2\_summary\_tokens\_train
count               38110.000000               38110.000000
mean                 1034.331068                  34.986119
std                   769.745530                  14.608020
min                    31.000000                   2.000000
25\%                   466.000000                  25.000000
50\%                   956.000000                  34.000000
75\%                  1415.000000                  43.000000
max                 12421.000000                 216.000000
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Conclusions: * Mean number of article tokens in the training set: \(1034\pm770\). 770 is a large standard deviation, hence the mean is not a very informative statistic (e.g., because of the influence of extreme values). The median may be more useful: \(956\) tokens.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Mean number of summary tokens in the training set: \(35\pm15\). The value of the median is very similar: \(34\).

\end{itemize}

\sphinxAtStartPar
Let us do the same for the RoBERTa LM, considering that the maximum number of tokens that the model can handle is two less than that reported of the model, being as we need to reserve two extra positions (or segments) for two special tokens. To double\sphinxhyphen{}check the maximum input of the length we can access to the \sphinxcode{\sphinxupquote{config.max\_position\_embeddings}} property of the RoBERTa HuggingFace checkpoint.

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[30]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{roberta} \PYG{o}{=} \PYG{n}{RobertaForMaskedLM}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[39]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens} \PYG{o}{=} \PYG{n}{RobertaForMaskedLM}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}\PYG{o}{.}\PYG{n}{config}\PYG{o}{.}\PYG{n}{max\PYGZus{}position\PYGZus{}embeddings} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} reserve tokens for [CLS] and [EOS]}
\PYG{n}{roberta\PYGZus{}valid\PYGZus{}train} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{roberta\PYGZus{}invalid\PYGZus{}train} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{roberta\PYGZus{}valid\PYGZus{}train}
\PYG{c+c1}{\PYGZsh{} out of the box... how many articles can be processed by the roberta model?}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Using the }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{checkpoint\PYGZus{}roberta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{ model, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}valid\PYGZus{}train}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ out of }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ training samples have \PYGZlt{}= }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ tokens and thus are subject to application out of the box, whereas }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}invalid\PYGZus{}train}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ will cause an error unless further measures are taken (e.g., via truncation of info. in a meaningful fashion).}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Using the "PlanTL-GOB-ES/roberta-base-bne" model, 10235 out of 38110 training samples have <= 512 tokens and thus are subject to application out of the box, whereas 27875 will cause an error unless further measures are taken (e.g., via truncation of info. in a meaningful fashion).
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[61]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}stats}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{,} \PYG{n}{roberta\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}
\noindent\sphinxincludegraphics[width=848\sphinxpxdimen,height=280\sphinxpxdimen]{{eda_29_0}.png}

\end{sphinxuseclass}
\end{sphinxuseclass}
\end{nbsphinxfancyoutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[69]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{p}{,}\PYG{n}{roberta\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{roberta\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[69]:\,\hspace{\fboxrule}\hspace{\fboxsep}}       roberta\_article\_tokens\_train  roberta\_summary\_tokens\_train
count                  38110.000000                  38110.000000
mean                    1035.331068                     35.986119
std                      769.745530                     14.608020
min                       32.000000                      3.000000
25\%                      467.000000                     26.000000
50\%                      957.000000                     35.000000
75\%                     1416.000000                     44.000000
max                    12422.000000                    217.000000
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Analogous conclusions can be inferred when using the RoBERTa tokenizer, with nearly negligible differences.


\subsection{Validation set}
\label{\detokenize{eda:Validation-set}}
\sphinxAtStartPar
Now, let us conduct this experiment on the validation and test datasets:

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[42]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{df\PYGZus{}val} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}json}\PYG{p}{(}\PYG{n}{VAL\PYGZus{}PATH}\PYG{p}{,} \PYG{n}{lines}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{summary}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[41]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}val}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}
\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}val}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{stderr}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[44]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{gpt2\PYGZus{}valid\PYGZus{}val} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{gpt2\PYGZus{}invalid\PYGZus{}val} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{gpt2\PYGZus{}valid\PYGZus{}val}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Using the }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{ model, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}valid\PYGZus{}val}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ out of }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ validation samples have \PYGZlt{}= }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ tokens and thus are subject to application out of the box, whereas }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}invalid\PYGZus{}val}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ will cause an error unless further measures are taken.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Using the "PlanTL-GOB-ES/gpt2-base-bne" model, 814 out of 4763 validation samples have <= 511 tokens and thus are subject to application out of the box, whereas 3949 will cause an error unless further measures are taken.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[70]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}stats}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{,} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}
\noindent\sphinxincludegraphics[width=856\sphinxpxdimen,height=280\sphinxpxdimen]{{eda_37_0}.png}

\end{sphinxuseclass}
\end{sphinxuseclass}
\end{nbsphinxfancyoutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[71]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[71]:\,\hspace{\fboxrule}\hspace{\fboxsep}}       gpt2\_article\_tokens\_val  gpt2\_summary\_tokens\_val
count              4763.000000              4763.000000
mean                950.719295                34.226748
std                 446.983980                11.507983
min                  75.000000                11.000000
25\%                 615.000000                25.000000
50\%                 951.000000                33.000000
75\%                1286.000000                42.000000
max                2038.000000                73.000000
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We can draw similar conclusions to those obtained for the training set: * Vast majority of the articles are around 600 to 1200 tokens long, whereas regular summary length revolves around 20 to 45 tokens. * Summary length seems to be independent from the length of the article. * Mean number of article tokens in the validation set: \(950\pm447\). Fairly close to the median: \(951\) tokens. * Mean number of summary tokens in the validation set: \(34\pm12\). The value of the median
is very similar: \(33\).

\sphinxAtStartPar
In order to avoid verbosity, we eschew reproducing these experiments using the RoBERTa tokenizer, since very similar results are to expect.

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[55]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}val} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}val}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\PYG{n}{roberta\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}val}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{stderr}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[46]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{roberta\PYGZus{}valid\PYGZus{}val} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{roberta\PYGZus{}invalid\PYGZus{}val} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{roberta\PYGZus{}valid\PYGZus{}val}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Using the }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{checkpoint\PYGZus{}roberta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{ model, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}valid\PYGZus{}val}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ out of }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}val}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ training samples have \PYGZlt{}= }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ tokens and thus are subject to application out of the box, whereas }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}invalid\PYGZus{}val}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ will cause an error unless further measures are taken.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Using the "PlanTL-GOB-ES/roberta-base-bne" model, 903 out of 4763 training samples have <= 512 tokens and thus are subject to application out of the box, whereas 3860 will cause an error unless further measures are taken.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Test set}
\label{\detokenize{eda:Test-set}}
\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[48]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{df\PYGZus{}test} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}json}\PYG{p}{(}\PYG{n}{TEST\PYGZus{}PATH}\PYG{p}{,} \PYG{n}{lines}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{summary}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[49]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}
\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{stderr}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[50]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{gpt2\PYGZus{}valid\PYGZus{}test} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{gpt2\PYGZus{}invalid\PYGZus{}test} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{gpt2\PYGZus{}valid\PYGZus{}val}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Using the }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{checkpoint\PYGZus{}gpt2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{ model, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}valid\PYGZus{}test}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ out of }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ test samples have \PYGZlt{}= }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ tokens and thus are subject to application out of the box, whereas }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{gpt2\PYGZus{}invalid\PYGZus{}test}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ will cause an error unless further measures are taken.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Using the "PlanTL-GOB-ES/gpt2-base-bne" model, 804 out of 4763 test samples have <= 511 tokens and thus are subject to application out of the box, whereas 3949 will cause an error unless further measures are taken.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[72]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}stats}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{,} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}
\noindent\sphinxincludegraphics[width=854\sphinxpxdimen,height=280\sphinxpxdimen]{{eda_46_0}.png}

\end{sphinxuseclass}
\end{sphinxuseclass}
\end{nbsphinxfancyoutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[73]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[73]:\,\hspace{\fboxrule}\hspace{\fboxsep}}       gpt2\_article\_tokens\_test  gpt2\_summary\_tokens\_test
count               4763.000000               4763.000000
mean                 948.605921                 34.231787
std                  444.319426                 11.396173
min                   75.000000                 11.000000
25\%                  616.000000                 26.000000
50\%                  951.000000                 33.000000
75\%                 1283.000000                 42.000000
max                 2002.000000                 73.000000
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We can draw similar conclusions to those obtained for the training and validation set, which enable us to assert that, overall, \sphinxstylestrong{the number of article and summary tokens across the different datasets are independent and identically distributed}: * Vast majority of the articles are around 600 to 1200 tokens long, whereas regular summary length revolves around 20 to 45 tokens. * Summary length seems to be independent from the length of the article. * Mean number of article tokens in the test
set: \(959\pm444\). Fairly close to the median: \(951\) tokens. * Mean number of summary tokens in the test set: \(34\pm11\). The value of the median is very similar: \(33\).

\sphinxAtStartPar
In order to avoid verbosity, we eschew reproducing these experiments using the RoBERTa tokenizer, since very similar results are to expect.

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[56]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}test} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\PYG{n}{roberta\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test} \PYG{o}{=} \PYG{n}{calc\PYGZus{}document\PYGZus{}tokens}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}roberta}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{nbsphinx-stderr}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{stderr}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 512). Running this sequence through the model will result in indexing errors
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[52]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{roberta\PYGZus{}valid\PYGZus{}test} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{\PYGZlt{}}\PYG{o}{=}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{roberta\PYGZus{}invalid\PYGZus{}test} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{roberta\PYGZus{}valid\PYGZus{}test}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Using the }\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{checkpoint\PYGZus{}roberta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZdq{}}\PYG{l+s+s1}{ model, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}valid\PYGZus{}test}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ out of }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{roberta\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ training samples have \PYGZlt{}= }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ tokens and thus are subject to application out of the box, whereas }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{roberta\PYGZus{}invalid\PYGZus{}test}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ will cause an error unless further measures are taken.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
Using the "PlanTL-GOB-ES/roberta-base-bne" model, 897 out of 4763 training samples have <= 512 tokens and thus are subject to application out of the box, whereas 3866 will cause an error unless further measures are taken.
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Recap}
\label{\detokenize{eda:Recap}}
\sphinxAtStartPar
Let us bring all data together to draw the final conclusions. To that end, and considering differences between the number of subwords using the different tokenizers, we will repeat the previous process using the training, validation and test sets altogether.

\begin{sphinxuseclass}{nbinput}
\begin{sphinxuseclass}{nblast}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[78]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens} \PYG{o}{=} \PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}train} \PYG{o}{+} \PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}val} \PYG{o}{+} \PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens\PYGZus{}test}
\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens} \PYG{o}{=} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}train} \PYG{o}{+} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}val} \PYG{o}{+} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens\PYGZus{}test}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[79]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plot\PYGZus{}stats}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens}\PYG{p}{,} \PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}
\noindent\sphinxincludegraphics[width=848\sphinxpxdimen,height=280\sphinxpxdimen]{{eda_54_0}.png}

\end{sphinxuseclass}
\end{sphinxuseclass}
\end{nbsphinxfancyoutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[87]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{df\PYGZus{}gpt2\PYGZus{}tokens} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens}\PYG{p}{,}\PYG{n}{gpt2\PYGZus{}summary\PYGZus{}tokens}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}article\PYGZus{}tokens}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{gpt2\PYGZus{}summary\PYGZus{}tokens}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}gpt2\PYGZus{}tokens}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[87]:\,\hspace{\fboxrule}\hspace{\fboxsep}}       gpt2\_article\_tokens  gpt2\_summary\_tokens
count         47636.000000         47636.000000
mean           1017.399509            34.834768
std             717.547947            14.036877
min              31.000000             2.000000
25\%             506.000000            25.000000
50\%             955.000000            34.000000
75\%            1379.000000            43.000000
max           12421.000000           216.000000
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}\begin{itemize}
\item {} 
\sphinxAtStartPar
Vast majority of the articles are around 500 to 1300 tokens long, whereas regular summary length revolves around 25 to 45 tokens.

\item {} 
\sphinxAtStartPar
Summary length seems to be independent from the length of the article.

\item {} 
\sphinxAtStartPar
Mean number of article tokens in the training set: \(1017\pm718\). Fairly close to the median: \(955\) tokens.

\item {} 
\sphinxAtStartPar
Mean number of summary tokens in the training set: \(35\pm14\). The value of the median is very similar: \(34\).

\end{itemize}

\sphinxAtStartPar
Some implications of these statistics is that when fine\sphinxhyphen{}tuning our models, no matter the data that is used to (i) train the models, (ii) validate or supervise the training process, and to (iii) assess goodness of the resulting models, if we aim at maximising performance metrics like ROUGE, one thing to bear in mind is that the summaries should have a length of around \(35\pm14\) tokens. This is essentially due to the fact that, even if we solely use samples that can be entirely fitted into
the model (i.e., without the need to trim/remove information from the original article and/or summary), the extent of the summary in terms of no. of subwords is independent of the size of the article. We can, indeed, double check it ourselves:

\begin{sphinxuseclass}{nbinput}
{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[90]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{df\PYGZus{}gpt2\PYGZus{}tokens}\PYG{p}{[}\PYG{n}{df\PYGZus{}gpt2\PYGZus{}tokens}\PYG{o}{.}\PYG{n}{gpt2\PYGZus{}article\PYGZus{}tokens} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{gpt2\PYGZus{}max\PYGZus{}n\PYGZus{}tokens}\PYG{p}{]}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

\end{sphinxuseclass}
\begin{sphinxuseclass}{nboutput}
\begin{sphinxuseclass}{nblast}
{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxuseclass}{output_area}
\begin{sphinxuseclass}{}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[90]:\,\hspace{\fboxrule}\hspace{\fboxsep}}       gpt2\_article\_tokens  gpt2\_summary\_tokens
count         12063.000000         12063.000000
mean            231.739120            34.641383
std             131.832378            11.038794
min              31.000000             4.000000
25\%             126.000000            27.000000
50\%             180.000000            34.000000
75\%             337.000000            41.000000
max             512.000000            98.000000
\end{sphinxVerbatim}



\end{sphinxuseclass}
\end{sphinxuseclass}
}

\end{sphinxuseclass}
\end{sphinxuseclass}

\chapter{GPT2Summarizer (\sphinxstyleliteralintitle{\sphinxupquote{gpt2\_summarizer.py}})}
\label{\detokenize{code:module-gpt2_summarizer}}\label{\detokenize{code:gpt2summarizer-gpt2-summarizer-py}}\label{\detokenize{code::doc}}\index{module@\spxentry{module}!gpt2\_summarizer@\spxentry{gpt2\_summarizer}}\index{gpt2\_summarizer@\spxentry{gpt2\_summarizer}!module@\spxentry{module}}\index{GPT2Summarizer (class in gpt2\_summarizer)@\spxentry{GPT2Summarizer}\spxextra{class in gpt2\_summarizer}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gpt2\_summarizer.}}\sphinxbfcode{\sphinxupquote{GPT2Summarizer}}}{\emph{\DUrole{n}{checkpoint\_name}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_train\_epochs}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{gradient\_accumulation\_steps}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_workers}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{device}\DUrole{p}{:} \DUrole{n}{torch.device}}, \emph{\DUrole{n}{output\_dir}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}}{}
\sphinxAtStartPar
This class serves as a common interface for GPT\sphinxhyphen{}2 based fine\sphinxhyphen{}tuned architectures
for abstractive summarization, both at training and inference time.
\subsubsection*{Notes}

\sphinxAtStartPar
This class is not meant to be instantiated \sphinxhyphen{} use instead the specialized subclasses defined for
training and inference accordingly.
\index{\_\_init\_\_() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{\_\_init\_\_()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{checkpoint\_name}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_train\_epochs}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{gradient\_accumulation\_steps}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_workers}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{device}\DUrole{p}{:} \DUrole{n}{torch.device}}, \emph{\DUrole{n}{output\_dir}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Constructor of a \sphinxtitleref{GPT2Summarizer} instance. This method is solely meant to be
invoked by the subclasses implementing this interface.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{checkpoint\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Model id of a pretrained HuggingFace Transformer hosted inside a model repo on
huggingface.co

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Training batch size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_train\_epochs}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of training epochs

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gradient\_accumulation\_steps}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of gradient accumulation steps

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_workers}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of workers available

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.device}}) \textendash{} torch.device object representing the device on which a torch.Tensor is or
will be allocated.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{output\_dir}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Output directory whereby outputs generated at training are stored.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_config\_file (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{\_config\_file}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer._config_file}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{\_config\_file}}}
\sphinxAtStartPar
Filepath whereby the configuration file of a fine\sphinxhyphen{}tuned model is to be loaded/stored
from.
\begin{quote}\begin{description}
\item[{Raises}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{NotImplementedError}} \textendash{} To be implemented by all subclasses.

\end{description}\end{quote}

\end{fulllineitems}

\index{\_model\_file (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{\_model\_file}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer._model_file}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{\_model\_file}}}
\sphinxAtStartPar
Filepath whereby a model is to be loaded/store from.
\begin{quote}\begin{description}
\item[{Raises}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{NotImplementedError}} \textendash{} To be implemented by all subclasses.

\end{description}\end{quote}

\end{fulllineitems}

\index{attrs\_to\_str() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{attrs\_to\_str()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.attrs_to_str}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attrs\_to\_str}}}{}{}
\sphinxAtStartPar
Meaningful string\sphinxhyphen{}like representation of the training attributes to fine\sphinxhyphen{}tune the model,
serving as a straightforward and effective fashion to uniquely identify the model.
\begin{quote}\begin{description}
\item[{Raises}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{NotImplementedError}} \textendash{} To be implemented by all subclasses.

\end{description}\end{quote}

\end{fulllineitems}

\index{batch\_size (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{batch\_size}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.batch_size}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{batch\_size}}\sphinxbfcode{\sphinxupquote{: int}}}
\sphinxAtStartPar
Training batch size, i.e., number of samples processed in parallel by
the model.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Training batch size

\item[{Return type}] \leavevmode
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{beam\_search() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{beam\_search()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.beam_search}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{beam\_search}}}{\emph{\DUrole{n}{context}}, \emph{\DUrole{n}{max\_length}\DUrole{o}{=}\DUrole{default_value}{60}}, \emph{\DUrole{n}{beam\_size}\DUrole{o}{=}\DUrole{default_value}{4}}, \emph{\DUrole{n}{temperature}\DUrole{o}{=}\DUrole{default_value}{1}}}{}
\sphinxAtStartPar
Performs beam search from \sphinxtitleref{context}, generating up to \sphinxtitleref{max\_length} tokens and
keeping \sphinxtitleref{beam\_size} hypotheses at each generation step.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{context}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Context tokenized text

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, of the generated summary, by default 60

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{beam\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Keep the most likely \sphinxtitleref{beam\_size} of hypotheses at each generation step to
eventually choose the hypothesis that has the overall highest probability,
by default 4

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{temperature}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1{]}, where
values closer to 1 indicate less randomness

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar


\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxtitleref{beam\_size} generated sequences along with their respective scores

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
Original code by Rohit Kumar Singh:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\item {} 
\sphinxAtStartPar
\sphinxurl{https://github.com/SKRohit/Generating\_Text\_Summary\_With\_GPT2/blob/master/utils.py}

\end{enumerate}

\end{fulllineitems}

\index{generate\_beam\_sample() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{generate\_beam\_sample()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.generate_beam_sample}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_beam\_sample}}}{\emph{\DUrole{n}{context}}, \emph{\DUrole{n}{max\_length}\DUrole{o}{=}\DUrole{default_value}{60}}, \emph{\DUrole{n}{beam\_size}\DUrole{o}{=}\DUrole{default_value}{4}}, \emph{\DUrole{n}{temperature}\DUrole{o}{=}\DUrole{default_value}{1}}}{}
\sphinxAtStartPar
Generate summary from \sphinxtitleref{context} with a maximum length of \sphinxtitleref{max\_length} tokens using
beam search.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{context}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Context tokenized text

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, of the generated summary, by default 60

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{beam\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Keep the most likely \sphinxtitleref{beam\_size} of hypotheses at each time step to eventually
choose the hypothesis that has the overall highest probability, by default 4

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{temperature}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1{]}, where
values closer to 1 indicate less randomness

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
\sphinxtitleref{beam\_size} generated sequences sorted in decreasing order of score (larger
scores signify better hipothetical quality of summary)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\_type\_

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_sample() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{generate\_sample()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.generate_sample}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_sample}}}{\emph{\DUrole{n}{context}}, \emph{\DUrole{n}{max\_length}\DUrole{o}{=}\DUrole{default_value}{60}}, \emph{\DUrole{n}{temperature}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{top\_k}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{top\_p}\DUrole{o}{=}\DUrole{default_value}{0.5}}}{{ $\rightarrow$ str}}
\sphinxAtStartPar
Generate summary from \sphinxtitleref{context} with a maximum length of \sphinxtitleref{max\_length} tokens
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{context}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Context tokenized text

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, of the generated summary, by default 60

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{temperature}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1{]}, where
values closer to 1 indicate less randomness

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform top\sphinxhyphen{}k filtering (only if \sphinxtitleref{top\_k} \textgreater{} 0), by default 10

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_p}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform nucleus filtering (only if \sphinxtitleref{top\_p} \textgreater{} 0), by default 0.5

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Generated summary in plain text

\item[{Return type}] \leavevmode
\sphinxAtStartPar
str

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_sample\_huggingface() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{generate\_sample\_huggingface()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.generate_sample_huggingface}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_sample\_huggingface}}}{\emph{\DUrole{n}{context}}, \emph{\DUrole{n}{max\_length}\DUrole{o}{=}\DUrole{default_value}{60}}, \emph{\DUrole{o}{**}\DUrole{n}{gen\_kwargs}}}{}
\sphinxAtStartPar
Generate summary from \sphinxtitleref{context} with a maximum length of \sphinxtitleref{max\_length} tokens using
HuggingFace’s functions for text generation
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{context}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Context tokenized text

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, of the generated summary, by default 60

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Generated summary in plain text

\item[{Return type}] \leavevmode
\sphinxAtStartPar
str

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_summaries\_from\_dataset() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{generate\_summaries\_from\_dataset()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.generate_summaries_from_dataset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_summaries\_from\_dataset}}}{\emph{\DUrole{n}{test\_data\_dir}\DUrole{p}{:} \DUrole{n}{dataset.GPT2SumDataset}}, \emph{\DUrole{n}{max\_length}\DUrole{o}{=}\DUrole{default_value}{100}}, \emph{\DUrole{n}{temperature}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{top\_k}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{top\_p}\DUrole{o}{=}\DUrole{default_value}{0.5}}, \emph{\DUrole{n}{out\_path}\DUrole{p}{:} \DUrole{n}{Optional\DUrole{p}{{[}}Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}\DUrole{p}{{]}}} \DUrole{o}{=} \DUrole{default_value}{None}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Generate summaries from test data and store them in disk.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{test\_data\_dir}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Test data from which samples are withdrawn

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, of the generated summary, by default 100

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{temperature}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1{]}, where
values closer to 1 indicate less randomness

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform top\sphinxhyphen{}k filtering (only if \sphinxtitleref{top\_k} \textgreater{} 0), by default 0

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_p}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform nucleus filtering (only if \sphinxtitleref{top\_p} \textgreater{} 0), by default 0.0

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{out\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Custom filepath to store the generated summaries, by default None

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{gradient\_accumulation\_steps (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{gradient\_accumulation\_steps}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.gradient_accumulation_steps}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{gradient\_accumulation\_steps}}\sphinxbfcode{\sphinxupquote{: int}}}
\sphinxAtStartPar
Number of K mini\sphinxhyphen{}batches of size \sphinxtitleref{batch\_size} to run before performing a backward
pass.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Number of gradient accumulation steps

\item[{Return type}] \leavevmode
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{log\_generated\_summaries() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{log\_generated\_summaries()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.log_generated_summaries}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{log\_generated\_summaries}}}{\emph{\DUrole{n}{data}}, \emph{\DUrole{n}{num}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{eval\_step}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{max\_length}\DUrole{o}{=}\DUrole{default_value}{60}}, \emph{\DUrole{n}{temperature}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{top\_k}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{top\_p}\DUrole{o}{=}\DUrole{default_value}{0.5}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Log \sphinxtitleref{num} generated summaries from \sphinxtitleref{data}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Dataset from which samples are withdrawn

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} number of summaries to generate (and log), by default 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{eval\_step}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Whether to log the article and actual summary, by default False

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, of the generated summary, by default 100

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{temperature}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1{]}, where
values closer to 1 indicate less randomness

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform top\sphinxhyphen{}k filtering (only if \sphinxtitleref{top\_k} \textgreater{} 0), by default 10

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_p}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform nucleus filtering (only if \sphinxtitleref{top\_p} \textgreater{} 0), by default 0.5

\end{itemize}

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
Code adaptation by Rohit Kumar’s work:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\item {} 
\sphinxAtStartPar
\sphinxurl{https://github.com/SKRohit/Generating\_Text\_Summary\_With\_GPT2/blob/master/utils.py}

\end{enumerate}

\end{fulllineitems}

\index{num\_train\_epochs (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{num\_train\_epochs}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.num_train_epochs}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{num\_train\_epochs}}\sphinxbfcode{\sphinxupquote{: int}}}~\begin{description}
\item[{Number of training epochs, i.e., number of complete passes through the entire}] \leavevmode
\sphinxAtStartPar
training dataset.

\end{description}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Number of training epochs

\item[{Return type}] \leavevmode
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_workers (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{num\_workers}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.num_workers}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{num\_workers}}\sphinxbfcode{\sphinxupquote{: int}}}
\sphinxAtStartPar
Number of processing elements (typically in terms of number of CPU cores
available) at your disposal to process data loading in parallel. In practice,
\sphinxtitleref{num\_workers} equals the number of samples that can be loaded in parallel.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Number of workers

\item[{Return type}] \leavevmode
\sphinxAtStartPar
int

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
Multi\sphinxhyphen{}process data loading (pytorch): \sphinxurl{https://pytorch.org/docs/stable/data.html\#multi-process-data-loading}

\end{fulllineitems}

\index{output\_dir (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{output\_dir}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.output_dir}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{output\_dir}}\sphinxbfcode{\sphinxupquote{: pathlib.Path}}}
\sphinxAtStartPar
Path of the directory of the generated model and training statistics.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Directory whereby the trained models, along with their configuration and
other statistics are allocated

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Path

\end{description}\end{quote}

\end{fulllineitems}

\index{sample\_sequence() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{sample\_sequence()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.sample_sequence}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{sample\_sequence}}}{\emph{\DUrole{n}{context}}, \emph{\DUrole{n}{length}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{temperature}\DUrole{o}{=}\DUrole{default_value}{1}}, \emph{\DUrole{n}{top\_k}\DUrole{o}{=}\DUrole{default_value}{0}}, \emph{\DUrole{n}{top\_p}\DUrole{o}{=}\DUrole{default_value}{0.0}}}{{ $\rightarrow$ torch.Tensor}}
\sphinxAtStartPar
Generate \sphinxtitleref{length} new tokens based on a context (\sphinxtitleref{context}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{context}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Context tokenized text

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of tokens to generate

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{temperature}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1{]}, where
values closer to 1 indicate less randomness

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform top\sphinxhyphen{}k filtering (only if \sphinxtitleref{top\_k} \textgreater{} 0), by default 0

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_p}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Perform nucleus filtering (only if \sphinxtitleref{top\_p} \textgreater{} 0), by default 0.0

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Tensor containing the tokenized text of the context and the generated tokens

\item[{Return type}] \leavevmode
\sphinxAtStartPar
torch.Tensor

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
Original code by Thomas Wolf:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\item {} 
\sphinxAtStartPar
\sphinxurl{https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run\_generation.py}

\end{enumerate}

\end{fulllineitems}

\index{tokenize\_input() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{tokenize\_input()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.tokenize_input}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{tokenize\_input}}}{}{{ $\rightarrow$ Callable}}
\sphinxAtStartPar
Ensure text is tokenized before feeding it into the model for summary generation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{func}} (\sphinxstyleliteralemphasis{\sphinxupquote{Callable}}) \textendash{} Callable function

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Callable object with the appropiate parametrization

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Callable

\end{description}\end{quote}

\end{fulllineitems}

\index{tokenizer (gpt2\_summarizer.GPT2Summarizer property)@\spxentry{tokenizer}\spxextra{gpt2\_summarizer.GPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.tokenizer}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{tokenizer}}}
\sphinxAtStartPar
HuggingFace Tokenizer object, which is targeted at preparing the inputs for a model.
By default, the tokenizer to use is that of the checkpoint (pre\sphinxhyphen{}trained model)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Tokenizer for the model

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Any

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
Documentation of HuggingFace Tokenizer class: \sphinxurl{https://huggingface.co/docs/transformers/main\_classes/tokenizer}

\end{fulllineitems}

\index{top\_k\_top\_p\_filtering() (gpt2\_summarizer.GPT2Summarizer method)@\spxentry{top\_k\_top\_p\_filtering()}\spxextra{gpt2\_summarizer.GPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer.GPT2Summarizer.top_k_top_p_filtering}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{top\_k\_top\_p\_filtering}}}{\emph{\DUrole{n}{logits}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{top\_k}\DUrole{o}{=}\DUrole{default_value}{0}}, \emph{\DUrole{n}{top\_p}\DUrole{o}{=}\DUrole{default_value}{0.0}}, \emph{\DUrole{n}{filter\_value}\DUrole{o}{=}\DUrole{default_value}{\sphinxhyphen{} inf}}}{{ $\rightarrow$ torch.Tensor}}
\sphinxAtStartPar
Filter a distribution of logits using top\sphinxhyphen{}k and/or nucleus (top\sphinxhyphen{}p) filtering
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logits}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Logits distribution shape (vocabulary size)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Keep only top k tokens with highest probability (top\sphinxhyphen{}k filtering), by default 0.
Top\sphinxhyphen{}k filtering is performed for \sphinxtitleref{top\_k} \textgreater{} 0

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{top\_p}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Keep the top tokens with cumulative probability \textgreater{}= top\_p (nucleus filtering), by default 0.0.
Nucleus filtering is performed for \sphinxtitleref{top\_p} \textgreater{} 0

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{filter\_value}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logits filter value, by default \sphinxhyphen{}float(‘Inf’)

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Filtered distribution of logits

\item[{Return type}] \leavevmode
\sphinxAtStartPar
torch.Tensor

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
Original code by Thomas Wolf:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{(}{)}%
\item {} 
\sphinxAtStartPar
\sphinxurl{https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317}

\item {} 
\sphinxAtStartPar
\sphinxurl{https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run\_generation.py}

\end{enumerate}

\end{fulllineitems}


\end{fulllineitems}



\chapter{TrainGPT2Summarizer (\sphinxstyleliteralintitle{\sphinxupquote{gpt2\_summarizer\_train.py}})}
\label{\detokenize{code:module-gpt2_summarizer_train}}\label{\detokenize{code:traingpt2summarizer-gpt2-summarizer-train-py}}\index{module@\spxentry{module}!gpt2\_summarizer\_train@\spxentry{gpt2\_summarizer\_train}}\index{gpt2\_summarizer\_train@\spxentry{gpt2\_summarizer\_train}!module@\spxentry{module}}\index{TrainGPT2Summarizer (class in gpt2\_summarizer\_train)@\spxentry{TrainGPT2Summarizer}\spxextra{class in gpt2\_summarizer\_train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{gpt2\_summarizer\_train.}}\sphinxbfcode{\sphinxupquote{TrainGPT2Summarizer}}}{\emph{\DUrole{n}{checkpoint\_name}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{data\_dir}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_train\_epochs}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{gradient\_accumulation\_steps}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{max\_grad\_norm}\DUrole{p}{:} \DUrole{n}{float}}, \emph{\DUrole{n}{lr}\DUrole{p}{:} \DUrole{n}{float}}, \emph{\DUrole{n}{n\_gpu}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_workers}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{device}\DUrole{p}{:} \DUrole{n}{torch.device}}, \emph{\DUrole{n}{output\_dir}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}, \emph{\DUrole{n}{seed}\DUrole{p}{:} \DUrole{n}{int}}}{}
\sphinxAtStartPar
This class provides a basic interface to train a GPT\sphinxhyphen{}2 based pre\sphinxhyphen{}trained model for
abstractive summarization, whereby fine\sphinxhyphen{}tuning can be simply achieved by instantiating
a class object and subsequently invoking the \sphinxtitleref{train} function.
\index{\_\_init\_\_() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{\_\_init\_\_()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{checkpoint\_name}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{data\_dir}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_train\_epochs}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{gradient\_accumulation\_steps}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{max\_grad\_norm}\DUrole{p}{:} \DUrole{n}{float}}, \emph{\DUrole{n}{lr}\DUrole{p}{:} \DUrole{n}{float}}, \emph{\DUrole{n}{n\_gpu}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{num\_workers}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{device}\DUrole{p}{:} \DUrole{n}{torch.device}}, \emph{\DUrole{n}{output\_dir}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}, \emph{\DUrole{n}{seed}\DUrole{p}{:} \DUrole{n}{int}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Constructor of a \sphinxtitleref{TrainGPT2Summarizer} instance, which provides all necessary functionality
to allow for the fine\sphinxhyphen{}grained tuning of a GPT2\sphinxhyphen{}like architecture for abstractive summarization.
A prior step to train any model is to have data well formatted. To that end, please refer to
the \sphinxtitleref{prepare\_data.py} module and its documentation, should you require it.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{checkpoint\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Model id of a pretrained HuggingFace Transformer hosted inside a model repo on
huggingface.co

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data\_dir}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Parent directory containing at least the training and validation datasets to fine
tune the model. The data should be formatted in such way that it can be processed
by a \sphinxtitleref{GPT2SumDataset} object. Refer to the \sphinxtitleref{prepare\_data.py} script for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Training batch size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_train\_epochs}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of training epochs

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gradient\_accumulation\_steps}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of gradient accumulation steps

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_grad\_norm}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} Max norm of the gradients. This helps leveraging the exploding gradients problem, whereby
large gradient vectors are rescaled so that their norm is at most \sphinxtitleref{max\_grad\_norm}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lr}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} Initial learning rate

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{n\_gpu}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of GPUs available

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_workers}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of workers available

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{device}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.device}}) \textendash{} torch.device object representing the device on which a torch.Tensor is or
will be allocated.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{output\_dir}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Output directory whereby outputs generated at training are stored.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Initialization state of a pseudo\sphinxhyphen{}random number generator to grant reproducibility of
the experiments

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_config\_file (gpt2\_summarizer\_train.TrainGPT2Summarizer property)@\spxentry{\_config\_file}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer._config_file}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{\_config\_file}}\sphinxbfcode{\sphinxupquote{: pathlib.Path}}}
\sphinxAtStartPar
Output filepath of the configuration file (in \sphinxtitleref{json} format) of the trained model. This file
is dumped to the output directory (refer to \sphinxtitleref{self.output\_dir} prop.), and named after the “named”
parameters utilized at training (refer to \sphinxtitleref{self.attrs\_to\_str()} module).
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Output filepath for the configuration file of the fine\sphinxhyphen{}tuned model

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Path

\end{description}\end{quote}

\end{fulllineitems}

\index{\_model\_file (gpt2\_summarizer\_train.TrainGPT2Summarizer property)@\spxentry{\_model\_file}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer._model_file}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{\_model\_file}}\sphinxbfcode{\sphinxupquote{: pathlib.Path}}}
\sphinxAtStartPar
Output filepath for the trained model binary (in \sphinxtitleref{bin} format). This file is dumped to the
output directory (refer to \sphinxtitleref{self.output\_dir} prop.), and named after the “named” parameters
utilized at training (refer to \sphinxtitleref{self.attrs\_to\_str()} module).
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Output filepath for the trained model binary

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Path

\end{description}\end{quote}

\end{fulllineitems}

\index{\_save\_train\_stats() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{\_save\_train\_stats()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer._save_train_stats}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_save\_train\_stats}}}{\emph{\DUrole{n}{training\_stats}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{precision}\DUrole{o}{=}\DUrole{default_value}{4}}}{{ $\rightarrow$ pandas.core.frame.DataFrame}}
\sphinxAtStartPar
Save the statistics generated throughout the training process.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{training\_stats}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}) \textendash{} Collection of per\sphinxhyphen{}epoch statistics

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{precision}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of decimal places to use for floating\sphinxhyphen{}point valued fields, by
default 4

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Statistics generated throughout the training process arranged in a DataFrame

\item[{Return type}] \leavevmode
\sphinxAtStartPar
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{attrs\_to\_str() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{attrs\_to\_str()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.attrs_to_str}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attrs\_to\_str}}}{\emph{\DUrole{n}{add}\DUrole{p}{:} \DUrole{n}{Optional\DUrole{p}{{[}}str\DUrole{p}{{]}}} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{test\_data\_name}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}}{{ $\rightarrow$ str}}
\sphinxAtStartPar
Yield a string\sphinxhyphen{}like representation of the training attributes to fine\sphinxhyphen{}tune the model,
serving as a straightforward and effective fashion to uniquely identify the model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{add}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Substring to append at the end of the string model descriptor, by default None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{test\_data\_name}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Identifier of the test dataset to generate the necessary filepaths. By default “”

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Textual representation of the training parameters utilized for fine\sphinxhyphen{}tuning

\item[{Return type}] \leavevmode
\sphinxAtStartPar
str

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_loss() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{compute\_loss()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.compute_loss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{compute\_loss}}}{\emph{\DUrole{n}{logits}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{labels}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{sum\_idx}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}}{{ $\rightarrow$ float}}
\sphinxAtStartPar
Compute loss over the logits w.r.t. truth labels, considering to that end the subset of
scores yielded for reference summaries.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logits}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Raw, unnormalized scores outputted by the last layer of the model.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{labels}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Collection of labels (tokenized actual summaries)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sum\_idx}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Per sample article/summary separator index

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Computed loss

\item[{Return type}] \leavevmode
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{eval() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{eval()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.eval}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{eval}}}{}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}torch.Tensor\DUrole{p}{, }float\DUrole{p}{, }float\DUrole{p}{{]}}}}
\sphinxAtStartPar
Evaluate performance of the model on the validation set
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Perplexity of the model, average loss and elapsed time

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tuple{[}torch.Tensor, float, float{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{loss\_func (gpt2\_summarizer\_train.TrainGPT2Summarizer property)@\spxentry{loss\_func}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.loss_func}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{loss\_func}}\sphinxbfcode{\sphinxupquote{: Callable}}}
\sphinxAtStartPar
Function that computes the cross entropy loss between input and target,
ignoring the padding token.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Cross entropy loss function

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Callable

\end{description}\end{quote}

\end{fulllineitems}

\index{max\_grad\_norm (gpt2\_summarizer\_train.TrainGPT2Summarizer property)@\spxentry{max\_grad\_norm}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.max_grad_norm}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{max\_grad\_norm}}\sphinxbfcode{\sphinxupquote{: float}}}
\sphinxAtStartPar
Max norm of the gradients. This helps leveraging the exploding gradients
problem, whereby large gradient vectors are rescaled so that their norm is at
most \sphinxtitleref{max\_grad\_norm}.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Max norm of the gradients

\item[{Return type}] \leavevmode
\sphinxAtStartPar
float

\end{description}\end{quote}

\end{fulllineitems}

\index{save\_trained\_model() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{save\_trained\_model()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.save_trained_model}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save\_trained\_model}}}{\emph{\DUrole{n}{model\_file}\DUrole{p}{:} \DUrole{n}{Optional\DUrole{p}{{[}}Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}\DUrole{p}{{]}}} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{config\_file}\DUrole{p}{:} \DUrole{n}{Optional\DUrole{p}{{[}}Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}\DUrole{p}{{]}}} \DUrole{o}{=} \DUrole{default_value}{None}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Dump trained model (in \sphinxtitleref{bin} format) and the configuration parameters (in \sphinxtitleref{json} format)
of the GPT2 model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model\_file}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Custom ouput model filepath, if not specified, it is dumped to \sphinxtitleref{self.\_model\_file},
by default None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config\_file}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Custom ouput configuration filepath, if not specified, it is dumped to \sphinxtitleref{self.\_config\_file},
by default None

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (gpt2\_summarizer\_train.TrainGPT2Summarizer method)@\spxentry{train()}\spxextra{gpt2\_summarizer\_train.TrainGPT2Summarizer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:gpt2_summarizer_train.TrainGPT2Summarizer.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{num\_warmup\_steps}\DUrole{o}{=}\DUrole{default_value}{200}}, \emph{\DUrole{n}{num\_training\_steps}\DUrole{o}{=}\DUrole{default_value}{80000}}}{{ $\rightarrow$ pandas.core.frame.DataFrame}}
\sphinxAtStartPar
Train a GPT\sphinxhyphen{}like architecture to generate abstractive summaries utilizing the AdamW
(Decoupled Weight Decay Regularization) optimizer, gradient clipping, cross\sphinxhyphen{}entropy loss
and a linear scheduler, with a learning rate that decreases linearly from the initial lr
set in the optimizer to 0, after a warmup period (\sphinxtitleref{num\_warmup\_steps}) and during \sphinxtitleref{num\_training\_steps}
training steps, where the learning rate increases from 0 to the initial lr set in the
optimizer.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_warmup\_steps}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of steps for the warmup phase of the linear scheduler, by default 100

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_training\_steps}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Total number of training steps for the linear scheduler, by default 80000

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Statistics generated throughout the training process arranged in a DataFrame

\item[{Return type}] \leavevmode
\sphinxAtStartPar
pd.DataFrame

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
The code for this method is largely based on the following work:
(1) \sphinxurl{https://mccormickml.com/2019/07/22/BERT-fine-tuning/\#43-training-loop}
(2) \sphinxurl{https://colab.research.google.com/github/kozodoi/website/blob/master/\_notebooks/2021-02-19-gradient-accumulation.ipynb\#scrollTo=ISFvH2p8dqYQ}
(3) \sphinxurl{https://github.com/SKRohit/Generating\_Text\_Summary\_With\_GPT2/blob/master/train\_gpt2\_summarizer.py}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Utils (\sphinxstyleliteralintitle{\sphinxupquote{utils.py}})}
\label{\detokenize{code:module-utils}}\label{\detokenize{code:utils-utils-py}}\index{module@\spxentry{module}!utils@\spxentry{utils}}\index{utils@\spxentry{utils}!module@\spxentry{module}}\index{add\_special\_tokens() (in module utils)@\spxentry{add\_special\_tokens()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.add_special_tokens}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{add\_special\_tokens}}}{\emph{\DUrole{n}{tokenizer}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}PlanTL\sphinxhyphen{}GOB\sphinxhyphen{}ES/gpt2\sphinxhyphen{}base\sphinxhyphen{}bne\textquotesingle{}}}}{}
\sphinxAtStartPar
Returns GPT2 tokenizer after adding separator and padding tokens
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Model id of a pretrained HuggingFace tokenizer hosted inside a model repo on
huggingface.co , by default “PlanTL\sphinxhyphen{}GOB\sphinxhyphen{}ES/gpt2\sphinxhyphen{}base\sphinxhyphen{}bne”

\item[{Returns}] \leavevmode
\sphinxAtStartPar


\item[{Return type}] \leavevmode
\sphinxAtStartPar
GPT2 tokenizer after adding separator and padding tokens

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_rouge\_score() (in module utils)@\spxentry{compute\_rouge\_score()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.compute_rouge_score}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{compute\_rouge\_score}}}{\emph{\DUrole{n}{test\_summaries}\DUrole{p}{:} \DUrole{n}{Iterable}}, \emph{\DUrole{n}{generated}\DUrole{p}{:} \DUrole{n}{Iterable}}, \emph{\DUrole{n}{results\_path}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}, \emph{\DUrole{n}{score\_format}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}csv\textquotesingle{}}}}{}
\sphinxAtStartPar
Compute average ROUGE scores of \sphinxtitleref{generated} summaries, using \sphinxtitleref{test\_summaries} as reference.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{test\_summaries}} (\sphinxstyleliteralemphasis{\sphinxupquote{Iterable}}) \textendash{} Reference summaries

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{generated}} (\sphinxstyleliteralemphasis{\sphinxupquote{Iterable}}) \textendash{} Generated summaries

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{results\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Filepath to store ROUGE metrics

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{score\_format}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Format to store assessment results, by default “csv”

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{detokenize\_input() (in module utils)@\spxentry{detokenize\_input()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.detokenize_input}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{detokenize\_input}}}{\emph{\DUrole{n}{func}\DUrole{p}{:} \DUrole{n}{Callable}}}{{ $\rightarrow$ Callable}}
\sphinxAtStartPar
Detokenize text input, when required.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{func}} (\sphinxstyleliteralemphasis{\sphinxupquote{Callable}}) \textendash{} Callable function

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Callable object with the appropiate parametrization

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Callable

\end{description}\end{quote}

\end{fulllineitems}

\index{format\_time() (in module utils)@\spxentry{format\_time()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.format_time}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{format\_time}}}{\emph{\DUrole{n}{elapsed}\DUrole{p}{:} \DUrole{n}{float}}}{{ $\rightarrow$ str}}
\sphinxAtStartPar
Format time in seconds to hh:mm:ss time format.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{elapsed}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) \textendash{} Time in seconds

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Time formatted in hh:mm:ss

\item[{Return type}] \leavevmode
\sphinxAtStartPar
str

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_tokenized\_text() (in module utils)@\spxentry{get\_tokenized\_text()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.get_tokenized_text}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{get\_tokenized\_text}}}{\emph{\DUrole{n}{text}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{tokenizer}}, \emph{\DUrole{n}{convert\_ids\_to\_tokens}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ list}}
\sphinxAtStartPar
Returns tokenized text using the tokenizer \sphinxtitleref{tokenizer}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Text to tokenize

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxtitleref{PreTrainedTokenizer} or \sphinxtitleref{PreTrainedTokenizerFast}) \textendash{} Tokenizer

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{convert\_ids\_to\_tokens}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Whether to convert ids to tokens, by default False

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Tokenized text

\item[{Return type}] \leavevmode
\sphinxAtStartPar
list

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_serialized\_data() (in module utils)@\spxentry{load\_serialized\_data()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.load_serialized_data}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{load\_serialized\_data}}}{\emph{\DUrole{n}{filename}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{return\_dict\_values}\DUrole{p}{:} \DUrole{n}{bool} \DUrole{o}{=} \DUrole{default_value}{False}}}{{ $\rightarrow$ Union\DUrole{p}{{[}}dict\DUrole{p}{, }Any\DUrole{p}{{]}}}}
\sphinxAtStartPar
Utility to load serialized data (and other optional stored values)
from disk using \sphinxstyleemphasis{pickle}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Filename of the file to be loaded.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{return\_dict\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, returns the values just the values
of the dictionary containing all stored data, defaults to False.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Loaded data

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Union{[}dict, Any{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{makedir() (in module utils)@\spxentry{makedir()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.makedir}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{makedir}}}{\emph{\DUrole{n}{path}\DUrole{p}{:} \DUrole{n}{Union\DUrole{p}{{[}}str\DUrole{p}{, }pathlib.Path\DUrole{p}{{]}}}}, \emph{\DUrole{n}{remove\_filename}\DUrole{p}{:} \DUrole{n}{bool} \DUrole{o}{=} \DUrole{default_value}{False}}, \emph{\DUrole{n}{recursive}\DUrole{p}{:} \DUrole{n}{bool} \DUrole{o}{=} \DUrole{default_value}{True}}, \emph{\DUrole{n}{exist\_ok}\DUrole{p}{:} \DUrole{n}{bool} \DUrole{o}{=} \DUrole{default_value}{True}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Creates directory from path if not exists.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{Union}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{Path}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Path of the directory to be created.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{remove\_filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, it attempts to remove the filename from the path, defaults
to False

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recursive}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Creates directories recursively (i.e., create necessary subdirectories if
necessary), by default True

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{exist\_ok}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to False, it arises an error if \sphinxtitleref{path} directory exists, by default True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{prepare\_input\_summarizer() (in module utils)@\spxentry{prepare\_input\_summarizer()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.prepare_input_summarizer}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{prepare\_input\_summarizer}}}{\emph{\DUrole{n}{text}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{tokenizer}}, \emph{\DUrole{n}{max\_input}\DUrole{o}{=}\DUrole{default_value}{512}}, \emph{\DUrole{n}{gpt2\_summary\_length}\DUrole{p}{:} \DUrole{n}{int} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{as\_tokens}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{o}{**}\DUrole{n}{spacy\_kwargs}}}{}
\sphinxAtStartPar
Prepare input to be handled by the Transformer model
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Raw, unprocessed text

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxtitleref{PreTrainedTokenizer} or \sphinxtitleref{PreTrainedTokenizerFast}) \textendash{} Tokenizer object used to control for the number of tokens in \sphinxtitleref{text}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_input}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum length, in terms of tokens, that the input can handle, by default 512

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gpt2\_summary\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Length, in terms of tokens, reserved to generate a summary when using decoder\sphinxhyphen{}only
based summarizers (e.g., GPT\sphinxhyphen{}2), by default None. If you wish to use a different
architecture (e.g., encoder\sphinxhyphen{}decoder), do NOT specify this argument.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tokens}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Return the input as tokens (hence not as plain text).

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Prepared input to be handled by a Transformer and a flag indicating whether the text
has been trimmed.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
(Union{[}str,torch.Tensor{]}, bool)

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_seed() (in module utils)@\spxentry{set\_seed()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.set_seed}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{set\_seed}}}{\emph{\DUrole{n}{seed}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{gpu\_mode}\DUrole{p}{:} \DUrole{n}{bool}}}{}
\sphinxAtStartPar
Set initialization state of a pseudo\sphinxhyphen{}random number generator to grant reproducibility
of the experiments
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Seed

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gpu\_mode}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Whether there are GPU’s available

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{split\_text\_into\_sentences\_spacy() (in module utils)@\spxentry{split\_text\_into\_sentences\_spacy()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.split_text_into_sentences_spacy}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{split\_text\_into\_sentences\_spacy}}}{\emph{\DUrole{n}{text}}, \emph{\DUrole{n}{spacy\_model}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}es\_core\_news\_sm\textquotesingle{}}}}{}
\sphinxAtStartPar
Splits text into sentences using the Spacy library.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Text to be splitted into sentences.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{spacy\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{SpaCy pretrained model object}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} SpaCy pretrained model used to split text into sentences or pretrained
model identifier, defaults to ‘es\_core\_news\_sm’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
List of sentences in \sphinxtitleref{text}.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
list

\end{description}\end{quote}
\subsubsection*{Notes}

\sphinxAtStartPar
SpaCy builds a syntactic tree for each sentence, a robust method that yields
more statistical information about the text than NLTK. It performs substancially
better than NLTK when using not polished text.

\end{fulllineitems}

\index{store\_serialized\_data() (in module utils)@\spxentry{store\_serialized\_data()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:utils.store_serialized_data}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{store\_serialized\_data}}}{\emph{\DUrole{n}{data}}, \emph{\DUrole{n}{out\_filename}}, \emph{\DUrole{n}{protocol}\DUrole{p}{:} \DUrole{n}{int} \DUrole{o}{=} \DUrole{default_value}{5}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Utility to dump precomputed data to disk using \sphinxstyleemphasis{pickle}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{\_type\_}}) \textendash{} Data to serialize

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{out\_filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Path for the output file

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{protocol}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Protocol used for \sphinxstyleemphasis{pickle}, by default pickle.HIGHEST\_PROTOCOL

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{g}
\item\relax\sphinxstyleindexentry{gpt2\_summarizer}\sphinxstyleindexpageref{code:\detokenize{module-gpt2_summarizer}}
\item\relax\sphinxstyleindexentry{gpt2\_summarizer\_train}\sphinxstyleindexpageref{code:\detokenize{module-gpt2_summarizer_train}}
\indexspace
\bigletter{u}
\item\relax\sphinxstyleindexentry{utils}\sphinxstyleindexpageref{code:\detokenize{module-utils}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}