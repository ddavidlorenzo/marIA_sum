<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPT2Summarizer (gpt2_summarizer.py) &mdash; Implementation of a framework to fine-tune GPT/GPT2 based models on abstractive summarization.  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Exploratory Data Analysis of the XL-Sum dataset." href="eda.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Implementation of a framework to fine-tune GPT/GPT2 based models on abstractive summarization.
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="eda.html">Exploratory Data Analysis of the <em>XL-Sum</em> dataset.</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GPT2Summarizer (<code class="docutils literal notranslate"><span class="pre">gpt2_summarizer.py</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#module-gpt2_summarizer_train">TrainGPT2Summarizer (<code class="docutils literal notranslate"><span class="pre">gpt2_summarizer_train.py</span></code>)</a></li>
<li class="toctree-l1"><a class="reference internal" href="#module-utils">Utils (<code class="docutils literal notranslate"><span class="pre">utils.py</span></code>)</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Implementation of a framework to fine-tune GPT/GPT2 based models on abstractive summarization.</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>GPT2Summarizer (<code class="docutils literal notranslate"><span class="pre">gpt2_summarizer.py</span></code>)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/code.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="module-gpt2_summarizer">
<span id="gpt2summarizer-gpt2-summarizer-py"></span><h1>GPT2Summarizer (<code class="docutils literal notranslate"><span class="pre">gpt2_summarizer.py</span></code>)<a class="headerlink" href="#module-gpt2_summarizer" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">gpt2_summarizer.</span></span><span class="sig-name descname"><span class="pre">GPT2Summarizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_train_epochs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_accumulation_steps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer" title="Permalink to this definition"></a></dt>
<dd><p>This class serves as a common interface for GPT-2 based fine-tuned architectures
for abstractive summarization, both at training and inference time.</p>
<p class="rubric">Notes</p>
<p>This class is not meant to be instantiated - use instead the specialized subclasses defined for
training and inference accordingly.</p>
<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_train_epochs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_accumulation_steps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Constructor of a <cite>GPT2Summarizer</cite> instance. This method is solely meant to be
invoked by the subclasses implementing this interface.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_name</strong> (<em>str</em>) – Model id of a pretrained HuggingFace Transformer hosted inside a model repo on
huggingface.co</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Training batch size</p></li>
<li><p><strong>num_train_epochs</strong> (<em>int</em>) – Number of training epochs</p></li>
<li><p><strong>gradient_accumulation_steps</strong> (<em>int</em>) – Number of gradient accumulation steps</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers available</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – torch.device object representing the device on which a torch.Tensor is or
will be allocated.</p></li>
<li><p><strong>output_dir</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Path</em><em>]</em>) – Output directory whereby outputs generated at training are stored.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer._config_file">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">_config_file</span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer._config_file" title="Permalink to this definition"></a></dt>
<dd><p>Filepath whereby the configuration file of a fine-tuned model is to be loaded/stored
from.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – To be implemented by all subclasses.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer._model_file">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">_model_file</span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer._model_file" title="Permalink to this definition"></a></dt>
<dd><p>Filepath whereby a model is to be loaded/store from.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – To be implemented by all subclasses.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.attrs_to_str">
<span class="sig-name descname"><span class="pre">attrs_to_str</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.attrs_to_str" title="Permalink to this definition"></a></dt>
<dd><p>Meaningful string-like representation of the training attributes to fine-tune the model,
serving as a straightforward and effective fashion to uniquely identify the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – To be implemented by all subclasses.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.batch_size">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">batch_size</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.batch_size" title="Permalink to this definition"></a></dt>
<dd><p>Training batch size, i.e., number of samples processed in parallel by
the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Training batch size</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.beam_search">
<span class="sig-name descname"><span class="pre">beam_search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.beam_search" title="Permalink to this definition"></a></dt>
<dd><p>Performs beam search from <cite>context</cite>, generating up to <cite>max_length</cite> tokens and
keeping <cite>beam_size</cite> hypotheses at each generation step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>context</strong> (<em>array-like</em>) – Context tokenized text</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, of the generated summary, by default 60</p></li>
<li><p><strong>beam_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Keep the most likely <cite>beam_size</cite> of hypotheses at each generation step to
eventually choose the hypothesis that has the overall highest probability,
by default 4</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1], where
values closer to 1 indicate less randomness</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>beam_size</cite> generated sequences along with their respective scores</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Original code by Rohit Kumar Singh:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/utils.py">https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/utils.py</a></p></li>
</ol>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.generate_beam_sample">
<span class="sig-name descname"><span class="pre">generate_beam_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beam_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.generate_beam_sample" title="Permalink to this definition"></a></dt>
<dd><p>Generate summary from <cite>context</cite> with a maximum length of <cite>max_length</cite> tokens using
beam search.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>context</strong> (<em>array-like</em>) – Context tokenized text</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, of the generated summary, by default 60</p></li>
<li><p><strong>beam_size</strong> (<em>int</em><em>, </em><em>optional</em>) – Keep the most likely <cite>beam_size</cite> of hypotheses at each time step to eventually
choose the hypothesis that has the overall highest probability, by default 4</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1], where
values closer to 1 indicate less randomness</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>beam_size</cite> generated sequences sorted in decreasing order of score (larger
scores signify better hipothetical quality of summary)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>_type_</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.generate_sample">
<span class="sig-name descname"><span class="pre">generate_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.generate_sample" title="Permalink to this definition"></a></dt>
<dd><p>Generate summary from <cite>context</cite> with a maximum length of <cite>max_length</cite> tokens</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>context</strong> (<em>array-like</em>) – Context tokenized text</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, of the generated summary, by default 60</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1], where
values closer to 1 indicate less randomness</p></li>
<li><p><strong>top_k</strong> (<em>int</em><em>, </em><em>optional</em>) – Perform top-k filtering (only if <cite>top_k</cite> &gt; 0), by default 10</p></li>
<li><p><strong>top_p</strong> (<em>float</em><em>, </em><em>optional</em>) – Perform nucleus filtering (only if <cite>top_p</cite> &gt; 0), by default 0.5</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Generated summary in plain text</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.generate_sample_huggingface">
<span class="sig-name descname"><span class="pre">generate_sample_huggingface</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">gen_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.generate_sample_huggingface" title="Permalink to this definition"></a></dt>
<dd><p>Generate summary from <cite>context</cite> with a maximum length of <cite>max_length</cite> tokens using
HuggingFace’s functions for text generation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>context</strong> (<em>array-like</em>) – Context tokenized text</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, of the generated summary, by default 60</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Generated summary in plain text</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.generate_summaries_from_dataset">
<span class="sig-name descname"><span class="pre">generate_summaries_from_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_data_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">dataset.GPT2SumDataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.generate_summaries_from_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Generate summaries from test data and store them in disk.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_data_dir</strong> (<em>array-like</em>) – Test data from which samples are withdrawn</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, of the generated summary, by default 100</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1], where
values closer to 1 indicate less randomness</p></li>
<li><p><strong>top_k</strong> (<em>int</em><em>, </em><em>optional</em>) – Perform top-k filtering (only if <cite>top_k</cite> &gt; 0), by default 0</p></li>
<li><p><strong>top_p</strong> (<em>float</em><em>, </em><em>optional</em>) – Perform nucleus filtering (only if <cite>top_p</cite> &gt; 0), by default 0.0</p></li>
<li><p><strong>out_path</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Path</em><em>]</em><em>, </em><em>optional</em>) – Custom filepath to store the generated summaries, by default None</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.gradient_accumulation_steps">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">gradient_accumulation_steps</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.gradient_accumulation_steps" title="Permalink to this definition"></a></dt>
<dd><p>Number of K mini-batches of size <cite>batch_size</cite> to run before performing a backward
pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Number of gradient accumulation steps</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.log_generated_summaries">
<span class="sig-name descname"><span class="pre">log_generated_summaries</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">60</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.log_generated_summaries" title="Permalink to this definition"></a></dt>
<dd><p>Log <cite>num</cite> generated summaries from <cite>data</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>array-like</em>) – Dataset from which samples are withdrawn</p></li>
<li><p><strong>num</strong> (<em>int</em><em>, </em><em>optional</em>) – number of summaries to generate (and log), by default 1</p></li>
<li><p><strong>eval_step</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to log the article and actual summary, by default False</p></li>
<li><p><strong>max_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, of the generated summary, by default 100</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1], where
values closer to 1 indicate less randomness</p></li>
<li><p><strong>top_k</strong> (<em>int</em><em>, </em><em>optional</em>) – Perform top-k filtering (only if <cite>top_k</cite> &gt; 0), by default 10</p></li>
<li><p><strong>top_p</strong> (<em>float</em><em>, </em><em>optional</em>) – Perform nucleus filtering (only if <cite>top_p</cite> &gt; 0), by default 0.5</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Code adaptation by Rohit Kumar’s work:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/utils.py">https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/utils.py</a></p></li>
</ol>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.num_train_epochs">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">num_train_epochs</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.num_train_epochs" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Number of training epochs, i.e., number of complete passes through the entire</dt><dd><p>training dataset.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Number of training epochs</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.num_workers">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">num_workers</span></span><em class="property"><span class="pre">:</span> <span class="pre">int</span></em><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.num_workers" title="Permalink to this definition"></a></dt>
<dd><p>Number of processing elements (typically in terms of number of CPU cores
available) at your disposal to process data loading in parallel. In practice,
<cite>num_workers</cite> equals the number of samples that can be loaded in parallel.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Number of workers</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Multi-process data loading (pytorch): <a class="reference external" href="https://pytorch.org/docs/stable/data.html#multi-process-data-loading">https://pytorch.org/docs/stable/data.html#multi-process-data-loading</a></p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.output_dir">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">output_dir</span></span><em class="property"><span class="pre">:</span> <span class="pre">pathlib.Path</span></em><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.output_dir" title="Permalink to this definition"></a></dt>
<dd><p>Path of the directory of the generated model and training statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Directory whereby the trained models, along with their configuration and
other statistics are allocated</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Path</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.sample_sequence">
<span class="sig-name descname"><span class="pre">sample_sequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.sample_sequence" title="Permalink to this definition"></a></dt>
<dd><p>Generate <cite>length</cite> new tokens based on a context (<cite>context</cite>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>context</strong> (<em>array-like</em>) – Context tokenized text</p></li>
<li><p><strong>length</strong> (<em>int</em>) – Number of tokens to generate</p></li>
<li><p><strong>temperature</strong> (<em>int</em><em>, </em><em>optional</em>) – Introduce randomness of the predictions by scaling the model logits before
applying softmax, by default 1. Values for temperature range in (0,1], where
values closer to 1 indicate less randomness</p></li>
<li><p><strong>top_k</strong> (<em>int</em><em>, </em><em>optional</em>) – Perform top-k filtering (only if <cite>top_k</cite> &gt; 0), by default 0</p></li>
<li><p><strong>top_p</strong> (<em>float</em><em>, </em><em>optional</em>) – Perform nucleus filtering (only if <cite>top_p</cite> &gt; 0), by default 0.0</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor containing the tokenized text of the context and the generated tokens</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Original code by Thomas Wolf:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run_generation.py">https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run_generation.py</a></p></li>
</ol>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.tokenize_input">
<span class="sig-name descname"><span class="pre">tokenize_input</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.tokenize_input" title="Permalink to this definition"></a></dt>
<dd><p>Ensure text is tokenized before feeding it into the model for summary generation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func</strong> (<em>Callable</em>) – Callable function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Callable object with the appropiate parametrization</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.tokenizer">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">tokenizer</span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.tokenizer" title="Permalink to this definition"></a></dt>
<dd><p>HuggingFace Tokenizer object, which is targeted at preparing the inputs for a model.
By default, the tokenizer to use is that of the checkpoint (pre-trained model)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tokenizer for the model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Any</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Documentation of HuggingFace Tokenizer class: <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/tokenizer">https://huggingface.co/docs/transformers/main_classes/tokenizer</a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer.GPT2Summarizer.top_k_top_p_filtering">
<span class="sig-name descname"><span class="pre">top_k_top_p_filtering</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">inf</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#gpt2_summarizer.GPT2Summarizer.top_k_top_p_filtering" title="Permalink to this definition"></a></dt>
<dd><p>Filter a distribution of logits using top-k and/or nucleus (top-p) filtering</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<em>torch.Tensor</em>) – Logits distribution shape (vocabulary size)</p></li>
<li><p><strong>top_k</strong> (<em>int</em><em>, </em><em>optional</em>) – Keep only top k tokens with highest probability (top-k filtering), by default 0.
Top-k filtering is performed for <cite>top_k</cite> &gt; 0</p></li>
<li><p><strong>top_p</strong> (<em>float</em><em>, </em><em>optional</em>) – Keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering), by default 0.0.
Nucleus filtering is performed for <cite>top_p</cite> &gt; 0</p></li>
<li><p><strong>filter_value</strong> (<em>float</em><em>, </em><em>optional</em>) – Logits filter value, by default -float(‘Inf’)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Filtered distribution of logits</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Original code by Thomas Wolf:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317">https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317</a></p></li>
<li><p><a class="reference external" href="https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run_generation.py">https://github.com/huggingface/transformers/blob/5c3b32d44d0164aaa9b91405f48e53cf53a82b35/examples/run_generation.py</a></p></li>
</ol>
</dd></dl>

</dd></dl>

</section>
<section id="module-gpt2_summarizer_train">
<span id="traingpt2summarizer-gpt2-summarizer-train-py"></span><h1>TrainGPT2Summarizer (<code class="docutils literal notranslate"><span class="pre">gpt2_summarizer_train.py</span></code>)<a class="headerlink" href="#module-gpt2_summarizer_train" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">gpt2_summarizer_train.</span></span><span class="sig-name descname"><span class="pre">TrainGPT2Summarizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_train_epochs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_accumulation_steps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_grad_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_gpu</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer" title="Permalink to this definition"></a></dt>
<dd><p>This class provides a basic interface to train a GPT-2 based pre-trained model for
abstractive summarization, whereby fine-tuning can be simply achieved by instantiating
a class object and subsequently invoking the <cite>train</cite> function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_name</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_train_epochs</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_accumulation_steps</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_grad_norm</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_gpu</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.__init__" title="Permalink to this definition"></a></dt>
<dd><p>Constructor of a <cite>TrainGPT2Summarizer</cite> instance, which provides all necessary functionality
to allow for the fine-grained tuning of a GPT2-like architecture for abstractive summarization.
A prior step to train any model is to have data well formatted. To that end, please refer to
the <cite>prepare_data.py</cite> module and its documentation, should you require it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_name</strong> (<em>str</em>) – Model id of a pretrained HuggingFace Transformer hosted inside a model repo on
huggingface.co</p></li>
<li><p><strong>data_dir</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Path</em><em>]</em>) – Parent directory containing at least the training and validation datasets to fine
tune the model. The data should be formatted in such way that it can be processed
by a <cite>GPT2SumDataset</cite> object. Refer to the <cite>prepare_data.py</cite> script for further
information.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Training batch size</p></li>
<li><p><strong>num_train_epochs</strong> (<em>int</em>) – Number of training epochs</p></li>
<li><p><strong>gradient_accumulation_steps</strong> (<em>int</em>) – Number of gradient accumulation steps</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – Max norm of the gradients. This helps leveraging the exploding gradients problem, whereby
large gradient vectors are rescaled so that their norm is at most <cite>max_grad_norm</cite></p></li>
<li><p><strong>lr</strong> (<em>float</em>) – Initial learning rate</p></li>
<li><p><strong>n_gpu</strong> (<em>int</em>) – Number of GPUs available</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – Number of workers available</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – torch.device object representing the device on which a torch.Tensor is or
will be allocated.</p></li>
<li><p><strong>output_dir</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Path</em><em>]</em>) – Output directory whereby outputs generated at training are stored.</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – Initialization state of a pseudo-random number generator to grant reproducibility of
the experiments</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer._config_file">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">_config_file</span></span><em class="property"><span class="pre">:</span> <span class="pre">pathlib.Path</span></em><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer._config_file" title="Permalink to this definition"></a></dt>
<dd><p>Output filepath of the configuration file (in <cite>json</cite> format) of the trained model. This file
is dumped to the output directory (refer to <cite>self.output_dir</cite> prop.), and named after the “named”
parameters utilized at training (refer to <cite>self.attrs_to_str()</cite> module).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Output filepath for the configuration file of the fine-tuned model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Path</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer._model_file">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">_model_file</span></span><em class="property"><span class="pre">:</span> <span class="pre">pathlib.Path</span></em><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer._model_file" title="Permalink to this definition"></a></dt>
<dd><p>Output filepath for the trained model binary (in <cite>bin</cite> format). This file is dumped to the
output directory (refer to <cite>self.output_dir</cite> prop.), and named after the “named” parameters
utilized at training (refer to <cite>self.attrs_to_str()</cite> module).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Output filepath for the trained model binary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Path</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer._save_train_stats">
<span class="sig-name descname"><span class="pre">_save_train_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">training_stats</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">pandas.core.frame.DataFrame</span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer._save_train_stats" title="Permalink to this definition"></a></dt>
<dd><p>Save the statistics generated throughout the training process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>training_stats</strong> (<em>list</em>) – Collection of per-epoch statistics</p></li>
<li><p><strong>precision</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of decimal places to use for floating-point valued fields, by
default 4</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Statistics generated throughout the training process arranged in a DataFrame</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pd.DataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.attrs_to_str">
<span class="sig-name descname"><span class="pre">attrs_to_str</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">add</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_data_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.attrs_to_str" title="Permalink to this definition"></a></dt>
<dd><p>Yield a string-like representation of the training attributes to fine-tune the model,
serving as a straightforward and effective fashion to uniquely identify the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>add</strong> (<em>str</em><em>, </em><em>optional</em>) – Substring to append at the end of the string model descriptor, by default None</p></li>
<li><p><strong>test_data_name</strong> (<em>str</em>) – Identifier of the test dataset to generate the necessary filepaths. By default “”</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Textual representation of the training parameters utilized for fine-tuning</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.compute_loss">
<span class="sig-name descname"><span class="pre">compute_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sum_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.compute_loss" title="Permalink to this definition"></a></dt>
<dd><p>Compute loss over the logits w.r.t. truth labels, considering to that end the subset of
scores yielded for reference summaries.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> (<em>torch.Tensor</em>) – Raw, unnormalized scores outputted by the last layer of the model.</p></li>
<li><p><strong>labels</strong> (<em>torch.Tensor</em>) – Collection of labels (tokenized actual summaries)</p></li>
<li><p><strong>sum_idx</strong> (<em>torch.Tensor</em>) – Per sample article/summary separator index</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Computed loss</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">,</span> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.eval" title="Permalink to this definition"></a></dt>
<dd><p>Evaluate performance of the model on the validation set</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Perplexity of the model, average loss and elapsed time</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Tuple[torch.Tensor, float, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.loss_func">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">loss_func</span></span><em class="property"><span class="pre">:</span> <span class="pre">Callable</span></em><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.loss_func" title="Permalink to this definition"></a></dt>
<dd><p>Function that computes the cross entropy loss between input and target,
ignoring the padding token.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Cross entropy loss function</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.max_grad_norm">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">max_grad_norm</span></span><em class="property"><span class="pre">:</span> <span class="pre">float</span></em><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.max_grad_norm" title="Permalink to this definition"></a></dt>
<dd><p>Max norm of the gradients. This helps leveraging the exploding gradients
problem, whereby large gradient vectors are rescaled so that their norm is at
most <cite>max_grad_norm</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Max norm of the gradients</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.save_trained_model">
<span class="sig-name descname"><span class="pre">save_trained_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config_file</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.save_trained_model" title="Permalink to this definition"></a></dt>
<dd><p>Dump trained model (in <cite>bin</cite> format) and the configuration parameters (in <cite>json</cite> format)
of the GPT2 model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_file</strong> (<em>Union</em><em>[</em><em>str</em><em>,</em><em>Path</em><em>]</em><em>, </em><em>optional</em>) – Custom ouput model filepath, if not specified, it is dumped to <cite>self._model_file</cite>,
by default None</p></li>
<li><p><strong>config_file</strong> (<em>Union</em><em>[</em><em>str</em><em>,</em><em>Path</em><em>]</em><em>, </em><em>optional</em>) – Custom ouput configuration filepath, if not specified, it is dumped to <cite>self._config_file</cite>,
by default None</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="gpt2_summarizer_train.TrainGPT2Summarizer.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_warmup_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_training_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">80000</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">pandas.core.frame.DataFrame</span></span></span><a class="headerlink" href="#gpt2_summarizer_train.TrainGPT2Summarizer.train" title="Permalink to this definition"></a></dt>
<dd><p>Train a GPT-like architecture to generate abstractive summaries utilizing the AdamW
(Decoupled Weight Decay Regularization) optimizer, gradient clipping, cross-entropy loss
and a linear scheduler, with a learning rate that decreases linearly from the initial lr
set in the optimizer to 0, after a warmup period (<cite>num_warmup_steps</cite>) and during <cite>num_training_steps</cite>
training steps, where the learning rate increases from 0 to the initial lr set in the
optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_warmup_steps</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of steps for the warmup phase of the linear scheduler, by default 100</p></li>
<li><p><strong>num_training_steps</strong> (<em>int</em><em>, </em><em>optional</em>) – Total number of training steps for the linear scheduler, by default 80000</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Statistics generated throughout the training process arranged in a DataFrame</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>pd.DataFrame</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The code for this method is largely based on the following work:
(1) <a class="reference external" href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/#43-training-loop">https://mccormickml.com/2019/07/22/BERT-fine-tuning/#43-training-loop</a>
(2) <a class="reference external" href="https://colab.research.google.com/github/kozodoi/website/blob/master/_notebooks/2021-02-19-gradient-accumulation.ipynb#scrollTo=ISFvH2p8dqYQ">https://colab.research.google.com/github/kozodoi/website/blob/master/_notebooks/2021-02-19-gradient-accumulation.ipynb#scrollTo=ISFvH2p8dqYQ</a>
(3) <a class="reference external" href="https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/train_gpt2_summarizer.py">https://github.com/SKRohit/Generating_Text_Summary_With_GPT2/blob/master/train_gpt2_summarizer.py</a></p>
</dd></dl>

</dd></dl>

</section>
<section id="module-utils">
<span id="utils-utils-py"></span><h1>Utils (<code class="docutils literal notranslate"><span class="pre">utils.py</span></code>)<a class="headerlink" href="#module-utils" title="Permalink to this headline"></a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="utils.add_special_tokens">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">add_special_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'PlanTL-GOB-ES/gpt2-base-bne'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.add_special_tokens" title="Permalink to this definition"></a></dt>
<dd><p>Returns GPT2 tokenizer after adding separator and padding tokens</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tokenizer</strong> (<em>str</em><em>, </em><em>optional</em>) – Model id of a pretrained HuggingFace tokenizer hosted inside a model repo on
huggingface.co , by default “PlanTL-GOB-ES/gpt2-base-bne”</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>GPT2 tokenizer after adding separator and padding tokens</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.compute_rouge_score">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">compute_rouge_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_summaries</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generated</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Iterable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">results_path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'csv'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.compute_rouge_score" title="Permalink to this definition"></a></dt>
<dd><p>Compute average ROUGE scores of <cite>generated</cite> summaries, using <cite>test_summaries</cite> as reference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_summaries</strong> (<em>Iterable</em>) – Reference summaries</p></li>
<li><p><strong>generated</strong> (<em>Iterable</em>) – Generated summaries</p></li>
<li><p><strong>results_path</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Path</em><em>]</em>) – Filepath to store ROUGE metrics</p></li>
<li><p><strong>score_format</strong> (<em>str</em><em>, </em><em>optional</em>) – Format to store assessment results, by default “csv”</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.detokenize_input">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">detokenize_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="headerlink" href="#utils.detokenize_input" title="Permalink to this definition"></a></dt>
<dd><p>Detokenize text input, when required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func</strong> (<em>Callable</em>) – Callable function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Callable object with the appropiate parametrization</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.format_time">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">format_time</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">elapsed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#utils.format_time" title="Permalink to this definition"></a></dt>
<dd><p>Format time in seconds to hh:mm:ss time format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>elapsed</strong> (<em>float</em>) – Time in seconds</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Time formatted in hh:mm:ss</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.get_tokenized_text">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">get_tokenized_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_ids_to_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span></span></span><a class="headerlink" href="#utils.get_tokenized_text" title="Permalink to this definition"></a></dt>
<dd><p>Returns tokenized text using the tokenizer <cite>tokenizer</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<em>str</em>) – Text to tokenize</p></li>
<li><p><strong>tokenizer</strong> (<cite>PreTrainedTokenizer</cite> or <cite>PreTrainedTokenizerFast</cite>) – Tokenizer</p></li>
<li><p><strong>convert_ids_to_tokens</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to convert ids to tokens, by default False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tokenized text</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.load_serialized_data">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">load_serialized_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict_values</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#utils.load_serialized_data" title="Permalink to this definition"></a></dt>
<dd><p>Utility to load serialized data (and other optional stored values)
from disk using <em>pickle</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<em>str</em>) – Filename of the file to be loaded.</p></li>
<li><p><strong>return_dict_values</strong> (<em>bool</em><em>, </em><em>optional</em>) – If set to True, returns the values just the values
of the dictionary containing all stored data, defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Loaded data</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Union[dict, Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.makedir">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">makedir</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">pathlib.Path</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_filename</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recursive</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exist_ok</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#utils.makedir" title="Permalink to this definition"></a></dt>
<dd><p>Creates directory from path if not exists.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>Union</em><em>[</em><em>str</em><em>,</em><em>Path</em><em>]</em>) – Path of the directory to be created.</p></li>
<li><p><strong>remove_filename</strong> (<em>bool</em><em>, </em><em>optional</em>) – If set to True, it attempts to remove the filename from the path, defaults
to False</p></li>
<li><p><strong>recursive</strong> (<em>bool</em><em>, </em><em>optional</em>) – Creates directories recursively (i.e., create necessary subdirectories if
necessary), by default True</p></li>
<li><p><strong>exist_ok</strong> (<em>bool</em><em>, </em><em>optional</em>) – If set to False, it arises an error if <cite>path</cite> directory exists, by default True</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.prepare_input_summarizer">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">prepare_input_summarizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpt2_summary_length</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">as_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">spacy_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.prepare_input_summarizer" title="Permalink to this definition"></a></dt>
<dd><p>Prepare input to be handled by the Transformer model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<em>str</em>) – Raw, unprocessed text</p></li>
<li><p><strong>tokenizer</strong> (<cite>PreTrainedTokenizer</cite> or <cite>PreTrainedTokenizerFast</cite>) – Tokenizer object used to control for the number of tokens in <cite>text</cite></p></li>
<li><p><strong>max_input</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum length, in terms of tokens, that the input can handle, by default 512</p></li>
<li><p><strong>gpt2_summary_length</strong> (<em>int</em><em>, </em><em>optional</em>) – Length, in terms of tokens, reserved to generate a summary when using decoder-only
based summarizers (e.g., GPT-2), by default None. If you wish to use a different
architecture (e.g., encoder-decoder), do NOT specify this argument.</p></li>
<li><p><strong>as_tokens</strong> (<em>bool</em><em>, </em><em>optional</em>) – Return the input as tokens (hence not as plain text).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Prepared input to be handled by a Transformer and a flag indicating whether the text
has been trimmed.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(Union[str,torch.Tensor], bool)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.set_seed">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">set_seed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_mode</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.set_seed" title="Permalink to this definition"></a></dt>
<dd><p>Set initialization state of a pseudo-random number generator to grant reproducibility
of the experiments</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<em>int</em>) – Seed</p></li>
<li><p><strong>gpu_mode</strong> (<em>bool</em>) – Whether there are GPU’s available</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.split_text_into_sentences_spacy">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">split_text_into_sentences_spacy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spacy_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'es_core_news_sm'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#utils.split_text_into_sentences_spacy" title="Permalink to this definition"></a></dt>
<dd><p>Splits text into sentences using the Spacy library.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> (<em>str</em>) – Text to be splitted into sentences.</p></li>
<li><p><strong>spacy_model</strong> (<em>str</em><em> or </em><em>SpaCy pretrained model object</em><em>, </em><em>optional</em>) – SpaCy pretrained model used to split text into sentences or pretrained
model identifier, defaults to ‘es_core_news_sm’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of sentences in <cite>text</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>SpaCy builds a syntactic tree for each sentence, a robust method that yields
more statistical information about the text than NLTK. It performs substancially
better than NLTK when using not polished text.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="utils.store_serialized_data">
<span class="sig-prename descclassname"><span class="pre">utils.</span></span><span class="sig-name descname"><span class="pre">store_serialized_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">protocol</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#utils.store_serialized_data" title="Permalink to this definition"></a></dt>
<dd><p>Utility to dump precomputed data to disk using <em>pickle</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>_type_</em>) – Data to serialize</p></li>
<li><p><strong>out_filename</strong> (<em>str</em><em>, </em><em>optional</em>) – Path for the output file</p></li>
<li><p><strong>protocol</strong> (<em>int</em><em>, </em><em>optional</em>) – Protocol used for <em>pickle</em>, by default pickle.HIGHEST_PROTOCOL</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="eda.html" class="btn btn-neutral float-left" title="Exploratory Data Analysis of the XL-Sum dataset." accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, David Lorenzo Alfaro.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>